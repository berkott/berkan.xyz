<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="../favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,200..800;1,6..72,200..800&family=Roboto+Serif:ital,opsz,wght@0,8..144,100..900;1,8..144,100..900&family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
		
		<link href="../_app/immutable/assets/0.FjlhOvzP.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.C6bxIzgs.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B09LZa_R.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BuAUvOro.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B_P4XA_-.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.D3uzO-Do.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D8__v_G3.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/hqIbxRWK.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DKJMLC42.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DAMHlmQC.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BIEbWpQy.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.DiNTwiUB.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DhGDfn95.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/9.BhAYX86h.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BntcpoEZ.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/wXsJu9Ig.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BLKplkQ7.js">
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><div class="max-w-[100ch] mx-auto px-4 mb-10"><nav class="flex flex-col md:flex-row md:items-center md:justify-between py-4 border-b border-olive mb-3"><a href="/" class="text-olive font-bold text-2xl md:text-2xl no-underline hover:underline mb-4 md:mb-0">Berkan</a> <div class="flex flex-col md:flex-row md:space-x-6 space-y-4 md:space-y-0"><a href="/blog" class="text-olive no-underline hover:underline">Blog</a> <a href="/projects" class="text-olive no-underline hover:underline">Projects</a> <a href="/publications" class="text-olive no-underline hover:underline">Publications</a> <a href="/digitalGarden" class="text-olive no-underline hover:underline">Digital Garden</a></div></nav> <div><!----><p>Paper: <a href="https://arxiv.org/pdf/1711.00165" rel="nofollow">Deep Neural Networks as Gaussian Processes</a></p> <p>Set-up:</p> <ul><li>I’m using their notation, plus any that I define.</li> <li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mrow><mi>i</mi><mo separator="true">,</mo><mi>j</mi></mrow><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">W_{i,j}^l</annotation></semantics></math></span><!----></span> is drawn i.i.d. from some distribution with mean 0 and variance <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><mi mathvariant="normal">/</mi><msub><mi>N</mi><mi>l</mi></msub></mrow><annotation encoding="application/x-tex">\sigma_w^2 / N_l</annotation></semantics></math></span><!----></span> (TODO: Compare with NTK, muP, and other inits!).</li> <li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>b</mi><mi>i</mi><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">b_i^l</annotation></semantics></math></span><!----></span> is drawn i.i.d. from some distribution with mean 0 and variance <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma_b^2</annotation></semantics></math></span><!----></span>.</li></ul> <h1>Shallow NNGP</h1> <p>It was already known that in the infinite width limit, a one layer neural network (NN) with an i.i.d. prior over its parameters is equivalent to a Gaussian process (GP).</p> <ul><li><em>What does this actually mean?</em> It means that <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msubsup><mi>z</mi><mi>i</mi><mn>1</mn></msubsup><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>α</mi><mo>=</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msubsup><mi>z</mi><mi>i</mi><mn>1</mn></msubsup><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mi>α</mi><mo>=</mo><mi>k</mi></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(z_i^1(x^{\alpha = 1}), \ldots, z_i^1(x^{\alpha = k}))</annotation></semantics></math></span><!----></span>, for any <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mrow><mi>α</mi><mo>=</mo><mn>1</mn></mrow></msup><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><msup><mi>x</mi><mrow><mi>α</mi><mo>=</mo><mi>k</mi></mrow></msup></mrow><annotation encoding="application/x-tex">x^{\alpha = 1}, \ldots, x^{\alpha = k}</annotation></semantics></math></span><!----></span> has a joint multivariate Gaussian distribution. We can write <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mi>i</mi><mn>1</mn></msubsup><mo>∼</mo><mrow><mi mathvariant="script">G</mi><mi mathvariant="script">P</mi></mrow><mo stretchy="false">(</mo><msup><mi>μ</mi><mn>1</mn></msup><mo separator="true">,</mo><msup><mi>K</mi><mn>1</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">z_i^1 \sim \mathcal{GP} (\mu^1, K^1)</annotation></semantics></math></span><!----></span>. We assume the params to have 0 mean so <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>μ</mi><mn>1</mn></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\mu^1(x) = 0</annotation></semantics></math></span><!----></span>, and <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>K</mi><mn>1</mn></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>=</mo><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><mi mathvariant="double-struck">E</mi><mo stretchy="false">[</mo><mi>ϕ</mi><mo stretchy="false">(</mo><msubsup><mi>z</mi><mi>i</mi><mn>0</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi>ϕ</mi><mo stretchy="false">(</mo><msubsup><mi>z</mi><mi>i</mi><mn>0</mn></msubsup><mo stretchy="false">(</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo>=</mo><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><mi>C</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K^1(x ,x&#x27;) = \sigma_b^2 + \sigma_w^2 \mathbb{E}[\phi(z_i^0(x)) \phi(z_i^0(x&#x27;))] = \sigma_b^2 + \sigma_w^2 C(x, x&#x27;),</annotation></semantics></math></span><!----></div> where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mi>i</mi><mn>0</mn></msubsup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><msubsup><mi>b</mi><mi>j</mi><mn>0</mn></msubsup><mo>+</mo><msubsup><mi>W</mi><mi>j</mi><mn>0</mn></msubsup><mi>x</mi></mrow><annotation encoding="application/x-tex">z_i^0(x) = b_j^0 + W_j^0 x</annotation></semantics></math></span><!----></span>. This expectation is NOT over <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><!----></span>, it is just over the initialization of the parameters.</li> <li><em>How did they do this?</em> Central Limit Theorem (CLT) argument based on infinite width. This works because <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>i</mi><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">x^l_i</annotation></semantics></math></span><!----></span> and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mi>j</mi><mi>l</mi></msubsup></mrow><annotation encoding="application/x-tex">x^l_{j}</annotation></semantics></math></span><!----></span> are i.i.d. because deterministic functions of i.i.d. random variables are i.i.d.. Note this doesn’t assume the parameters are drawn from a Gaussian, it just assumes they are drawn i.i.d. from a distribution with finite mean and finite variance.</li></ul> <h1>Deep NNGP</h1> <p>The point of this paper is to extend the result to deep neural networks (DNNs). They do this by taking the hidden layer widths to infinity in succession (why does it matter that it’s in succession?). Recursively, we have</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>K</mi><mi>l</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>=</mo><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><msub><mi mathvariant="double-struck">E</mi><mrow><msubsup><mi>z</mi><mi>i</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo>∼</mo><mi>G</mi><mi>P</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><msup><mi>K</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">)</mo></mrow></msub><mo stretchy="false">[</mo><mi>ϕ</mi><mo stretchy="false">(</mo><msubsup><mi>z</mi><mi>i</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi>ϕ</mi><mo stretchy="false">(</mo><msubsup><mi>z</mi><mi>i</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msubsup><mo stretchy="false">(</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K^l(x ,x&#x27;) = \sigma_b^2 + \sigma_w^2 \mathbb{E}_{z_i^{l-1} \sim GP(0, K^{l-1})} [\phi(z_i^{l-1}(x)) \phi(z_i^{l-1}(x&#x27;))].</annotation></semantics></math></span><!----></div> <p>But of course, we only care about <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>z</mi><mi>i</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msubsup></mrow><annotation encoding="application/x-tex">z_i^{l-1}</annotation></semantics></math></span><!----></span> at <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><!----></span> and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">x&#x27;</annotation></semantics></math></span><!----></span>, so we can integrate against the joint at only those two points. We are left with a bivariate distribution with covariance matrix entries <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K^{l-1}(x, x)</annotation></semantics></math></span><!----></span>, <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K^{l-1}(x, x&#x27;)</annotation></semantics></math></span><!----></span>, and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K^{l-1}(x&#x27;, x&#x27;)</annotation></semantics></math></span><!----></span>. Thus, we can write</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>K</mi><mi>l</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>=</mo><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><msub><mi>F</mi><mi>ϕ</mi></msub><mo stretchy="false">[</mo><msup><mi>K</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>K</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo separator="true">,</mo><msup><mi>K</mi><mrow><mi>l</mi><mo>−</mo><mn>1</mn></mrow></msup><mo stretchy="false">(</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo stretchy="false">]</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K^l(x ,x&#x27;) = \sigma_b^2 + \sigma_w^2 F_\phi [K^{l-1}(x, x), K^{l-1}(x, x&#x27;), K^{l-1}(x&#x27;, x&#x27;)],</annotation></semantics></math></span><!----></div> <p>where F is a deterministic function whose form only depends on <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ϕ</mi></mrow><annotation encoding="application/x-tex">\phi</annotation></semantics></math></span><!----></span>. Assuming Gaussian initialization, the base case is the linear kernel (with bias) corresponding to the first layer</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msup><mi>K</mi><mn>0</mn></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>=</mo><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><mfrac><mrow><msup><mi>x</mi><mi mathvariant="normal">⊤</mi></msup><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><msub><mi>d</mi><mtext>in</mtext></msub></mfrac><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">K^0(x ,x&#x27;) = \sigma_b^2 + \sigma_w^2 \frac{x^\top x&#x27;}{d_\text{in}}.</annotation></semantics></math></span><!----></div> <h1>Prediction with an NNGP</h1> <p>See my <a href="/digitalGarden/gaussianProcesses">GPs</a> notes for how to do Bayesian prediction with GPs. Most notably, you can just do Gaussian process regression or kernelized ridge regression (KRR),</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>K</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">,</mo><mi>X</mi><mo stretchy="false">)</mo><mo>−</mo><msubsup><mi>σ</mi><mi>n</mi><mn>2</mn></msubsup><msub><mi>I</mi><mi>d</mi></msub><msup><mo stretchy="false">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>Y</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\hat{f}(x) = K(x, X) (K(X, X) - \sigma_n^2 I_{d} )^{-1} Y,</annotation></semantics></math></span><!----></div> <p>where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mi>n</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma_n^2</annotation></semantics></math></span><!----></span> is your ridge penalty / noise.</p> <h1>Simple example</h1> <h2>No hidden layers</h2> <p>If there are no hidden layers, our kernel is just the linear kernel (with a bias) and our NNGP is just ridge regression. With weight decay (l2 regularization) training the linear model with GD converges to the same solution (without l2 it converges to least squares).</p> <h2>One hidden layer</h2> <p>Ok now if we have one hidden layer and our activation function is <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>ReLU</mtext></mrow><annotation encoding="application/x-tex">\text{ReLU}</annotation></semantics></math></span><!----></span>, what happens? Our kernel is</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo><mo>=</mo><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>+</mo><mfrac><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><mrow><mn>2</mn><mi>π</mi></mrow></mfrac><msqrt><mrow><mo stretchy="false">(</mo><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><mfrac><mrow><mi mathvariant="normal">∥</mi><mi>x</mi><msup><mi mathvariant="normal">∥</mi><mn>2</mn></msup></mrow><msub><mi>d</mi><mtext>in</mtext></msub></mfrac><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><mfrac><mrow><mi mathvariant="normal">∥</mi><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mi mathvariant="normal">∥</mi><mn>2</mn></msup></mrow><msub><mi>d</mi><mtext>in</mtext></msub></mfrac><mo stretchy="false">)</mo></mrow></msqrt><mo stretchy="false">(</mo><mi>sin</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo>+</mo><mo stretchy="false">(</mo><mi>π</mi><mo>−</mo><mi>θ</mi><mo stretchy="false">)</mo><mi>cos</mi><mo>⁡</mo><mo stretchy="false">(</mo><mi>θ</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">K(x, x&#x27;) = \sigma_b^2 + \frac{\sigma_w^2}{2 \pi} \sqrt{(\sigma_b^2 + \sigma_w^2 \frac{\|x\|^2}{d_\text{in}}) (\sigma_b^2 + \sigma_w^2 \frac{\|x&#x27;\|^2}{d_\text{in}})} (\sin(\theta) + (\pi - \theta)\cos(\theta)),</annotation></semantics></math></span><!----></div> <p>where</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>θ</mi><mo>=</mo><msup><mrow><mi>cos</mi><mo>⁡</mo></mrow><mrow><mo>−</mo><mn>1</mn></mrow></msup><mo fence="false" stretchy="true" minsize="3em" maxsize="3em">(</mo><mfrac><mrow><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><mfrac><mrow><msup><mi>x</mi><mi mathvariant="normal">⊤</mi></msup><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><msub><mi>d</mi><mtext>in</mtext></msub></mfrac></mrow><msqrt><mrow><mo stretchy="false">(</mo><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><mfrac><mrow><mi mathvariant="normal">∥</mi><mi>x</mi><msup><mi mathvariant="normal">∥</mi><mn>2</mn></msup></mrow><msub><mi>d</mi><mtext>in</mtext></msub></mfrac><mo stretchy="false">)</mo><mo stretchy="false">(</mo><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup><mo>+</mo><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup><mfrac><mrow><mi mathvariant="normal">∥</mi><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><msup><mi mathvariant="normal">∥</mi><mn>2</mn></msup></mrow><msub><mi>d</mi><mtext>in</mtext></msub></mfrac><mo stretchy="false">)</mo></mrow></msqrt></mfrac><mo fence="false" stretchy="true" minsize="3em" maxsize="3em">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\theta = \cos^{-1} \Bigg( \frac{\sigma_b^2 + \sigma_w^2 \frac{x^\top x&#x27;}{d_\text{in}}}{\sqrt{(\sigma_b^2 + \sigma_w^2 \frac{\|x\|^2}{d_\text{in}}) (\sigma_b^2 + \sigma_w^2 \frac{\|x&#x27;\|^2}{d_\text{in}})}} \Bigg).</annotation></semantics></math></span><!----></div> <p>This is kinda ugly and IDK what to do with it. The <a href="/digitalGarden/kernelLimitations">limitations of kernels</a> results should hold. I ran a few inductive bias experiments to compare the NNGP with KRR to NNs with AdamW but they are not that interesting and I think they were a waste of time (see the dropdown below).</p> <details><summary>Inductive bias experiment</summary> Here's KRR with the one hidden layer ReLU NNGP and and train a one hidden layer ReLU NN to learn <!----><math><mrow><msup><mi>f</mi><mo>∗</mo></msup><mo form="prefix" stretchy="false">(</mo><mi>x</mi><mo form="postfix" stretchy="false">)</mo><mo>=</mo></mrow><mrow><msup><mi>x</mi><mn>2</mn></msup><mo>+</mo></mrow><mrow><mn>2</mn></mrow></math><!----> <!--[!--><!--]--><!----> with various numbers of training data points. All NNs trained to convergence. Weight decay in AdamW changes things, here I used 1e-6. Also, <!----><math><mrow><msub><mi>σ</mi><mi>w</mi></msub><mo>=</mo></mrow><mrow><msub><mi>σ</mi><mi>b</mi></msub><mo>=</mo></mrow><mrow><mn>1</mn></mrow></math><!----> <!--[!--><!--]--><!---->. <p><img src="/images/digitalGarden/NNGPInductiveBias.png" alt="NNGP inductive bias" title="NNGP inductive bias"></p> <p>TODO: Push python simulation code repo to github as well and link it on the digital garden home page.</p></details> <h1>Signal propagation</h1> <p>Deep signal propagation studies the statistics of hidden representation in deep NNs. They found some cool links to this work, most cleanly for tanh and also for ReLU.</p> <p>For tanh, the deep signal prop works identified an ordered and a chaotic phase, depending on <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma_w^2</annotation></semantics></math></span><!----></span> and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma_b^2</annotation></semantics></math></span><!----></span>. In the ordered phase, similar inputs to the NN yield similar outputs. This occurs when <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma_b^2</annotation></semantics></math></span><!----></span> dominates <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma_w^2</annotation></semantics></math></span><!----></span>. In the NNGP, this manifests as <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mi mathvariant="normal">∞</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K^\infty (x, x&#x27;)</annotation></semantics></math></span><!----></span> approaching a constant function. In the chaotic phase, similar inputs to the NN yield vastly different outputs. This occurs when <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mi>w</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma_w^2</annotation></semantics></math></span><!----></span> dominates <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mi>b</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma_b^2</annotation></semantics></math></span><!----></span>. In the NNGP, this manifests as <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mi mathvariant="normal">∞</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K^\infty (x, x)</annotation></semantics></math></span><!----></span> approaching a constant function and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mi mathvariant="normal">∞</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K^\infty (x, x&#x27;)</annotation></semantics></math></span><!----></span> approaching a smaller constant function. In other words, in the chaotic phase, the diagonal of the kernel matrix is some value and off diagonals are all some other, smaller, value.</p> <p>Interestingly, the NNGP performs best near the threshold between the chaotic and ordered phase. As depth increases, we converge towards <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mi mathvariant="normal">∞</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K^\infty (x, x&#x27;)</annotation></semantics></math></span><!----></span>, and only perform well closer and closer to the threshold. We do well at the threshold, because there, convergence to <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mi mathvariant="normal">∞</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><msup><mi>x</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">K^\infty (x, x&#x27;)</annotation></semantics></math></span><!----></span> is much slower (this is bc of some deep signal prop stuff I don’t understand).</p> <p><img src="/images/digitalGarden/NNGPSignalProp.png" alt="NNGP signal propagation" title="NNGP signal propagation"></p> <h1>Other experiments</h1> <p>They ran experiments (Figure 1) that showed on MNIST and CIFAR-10 NNs and NNGP do essentially equally well. This indicates that feature learning is not important to do well on MNIST and CIFAR-10! (TODO: Find similar experiment on ImageNet and other datasets).</p> <p><img src="/images/digitalGarden/NNGPNNPerformance.png" alt="NNGP and NN performance" title="NNGP and NN performance"></p> <p>Additionally, they ran experiments (Figure 2) that showed increasing width improves generalization for fully connected MLPs on CIFAR-10. TODO: why I should expect this?</p> <p><img src="/images/digitalGarden/NNWidthGeneralization.png" alt="NN generalization with width" title="NN generalization with width"></p> <p>They also show that the NNGP uncertainty is well correlated with empirical error on MNIST and CIFAR. It’s nice that you get uncertainty estimates for free.</p> <p>TODO: How computationally expensive is the NNGP?</p> <p>TODO: How does the NNGP compare to the NTK, RBF, and other kernels?</p><!----><!----></div></div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_1vrthg1 = {
						base: new URL("..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.C6bxIzgs.js"),
						import("../_app/immutable/entry/app.D3uzO-Do.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 9],
							data: [null,null],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
