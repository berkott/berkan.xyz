<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="../favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,200..800;1,6..72,200..800&family=Roboto+Serif:ital,opsz,wght@0,8..144,100..900;1,8..144,100..900&family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
		
		<link href="../_app/immutable/assets/0.FjlhOvzP.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.C6bxIzgs.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B09LZa_R.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BuAUvOro.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B_P4XA_-.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.D3uzO-Do.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D8__v_G3.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/hqIbxRWK.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DKJMLC42.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DAMHlmQC.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BIEbWpQy.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.DiNTwiUB.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DhGDfn95.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/12.KvsypP2y.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BntcpoEZ.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/wXsJu9Ig.js">
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><div class="max-w-[100ch] mx-auto px-4 mb-10"><nav class="flex flex-col md:flex-row md:items-center md:justify-between py-4 border-b border-olive mb-3"><a href="/" class="text-olive font-bold text-2xl md:text-2xl no-underline hover:underline mb-4 md:mb-0">Berkan</a> <div class="flex flex-col md:flex-row md:space-x-6 space-y-4 md:space-y-0"><a href="/blog" class="text-olive no-underline hover:underline">Blog</a> <a href="/projects" class="text-olive no-underline hover:underline">Projects</a> <a href="/publications" class="text-olive no-underline hover:underline">Publications</a> <a href="/digitalGarden" class="text-olive no-underline hover:underline">Digital Garden</a></div></nav> <div><!----><p>A Gaussian process (GP) is a stochastic process (a collection of random variables indexed by time or space), such that every finite collection of those random variables has a multivariate normal distribution (copied from <a href="https://en.wikipedia.org/wiki/Gaussian_process#:~:text=a%20Gaussian%20process%20is%20a%20stochastic%20process%20(a%20collection%20of%20random%20variables%20indexed%20by%20time%20or%20space)%2C%20such%20that%20every%20finite%20collection%20of%20those%20random%20variables%20has%20a%20multivariate%20normal%20distribution." rel="nofollow">Wikipedia</a>).</p> <p>Some GPs (specifically stationary GPs, meaning it has a constant mean and covariance only depends on relative position of data points), have an explicit representation, where you can write it as</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><mi>R</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">f(x) = g(R, x),</annotation></semantics></math></span><!----></div> <p>for some random variables <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi></mrow><annotation encoding="application/x-tex">R</annotation></semantics></math></span><!----></span> and some fixed deterministic function <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><!----></span> (I think this is true).</p> <h1>Bayesian inference and prediction with a GP</h1> <p>We have a training set <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="script">D</mi><mo>=</mo><mo stretchy="false">{</mo><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo separator="true">,</mo><mo>…</mo><mo separator="true">,</mo><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msup><mo separator="true">,</mo><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>n</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\mathcal{D} = \{ (x^{(1)}, y^{(1)}), \ldots, (x^{(n)}, y^{(n)}) \}</annotation></semantics></math></span><!----></span>, and we wish to make a Bayesian prediction (see my notes on <a href="/digitalGarden/bayesianStuff">Bayesian stuff</a> for the difference between Bayesian inference and prediction) on a test sample <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><!----></span> using our GP, which is a distribution over functions.</p> <p>We consider the following noise model, <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msup><mi>x</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo stretchy="false">)</mo><mo>+</mo><msup><mi>ϵ</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup></mrow><annotation encoding="application/x-tex">y^{(i)} = g(x^{(i)}) + \epsilon^{(i)}</annotation></semantics></math></span><!----></span>, where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ϵ</mi><mrow><mo stretchy="false">(</mo><mi>i</mi><mo stretchy="false">)</mo></mrow></msup><mo>∼</mo><mi mathvariant="script">N</mi><mo stretchy="false">(</mo><mn>0</mn><mo separator="true">,</mo><msubsup><mi>σ</mi><mi>n</mi><mn>2</mn></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\epsilon^{(i)} \sim \mathcal{N}(0, \sigma_n^2)</annotation></semantics></math></span><!----></span>.</p> <p>Our joint prior on labels is</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>Y</mi></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mi>f</mi></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>∼</mo><mi mathvariant="script">N</mi><mo fence="false" stretchy="true" minsize="1.8em" maxsize="1.8em">(</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>m</mi><mo stretchy="false">(</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>m</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo separator="true">,</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">,</mo><mi>X</mi><mo stretchy="false">)</mo><mo>+</mo><msubsup><mi>σ</mi><mi>n</mi><mn>2</mn></msubsup><msub><mi>I</mi><mi>d</mi></msub><mtext> </mtext></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>X</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>K</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo fence="false" stretchy="true" minsize="1.8em" maxsize="1.8em">)</mo><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">\begin{bmatrix} Y \\ f \end{bmatrix} \sim \mathcal{N} \Big(\begin{bmatrix} m(X) \\ m(x) \end{bmatrix}, \begin{bmatrix} K(X, X) + \sigma_n^2 I_d \ &amp; K(x, X) \\ K(x, X) &amp; K(x, x) \end{bmatrix} \Big).</annotation></semantics></math></span><!----></div> <p>From here, you just do the standard conditioning a joint Gaussian stuff (<a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Bayesian_inference:~:text=Conditional%20distributions" rel="nofollow">Wikipedia page for this</a>). Then you can write this as a new GP. This is your posterior predictive.</p> <p>You can place a GP prior over functions and then compute the posterior after seeing data to have some probability distribution for what the rest of the functin looks like. This is what they call Gaussian process regression. They get these cool visualizations from it like this from on <a href="https://scikit-learn.org/stable/modules/gaussian_process.html" rel="nofollow">scikit-learn</a> (this one uses the <code>ConstantKernel(1.0, constant_value_bounds="fixed") * RBF(1.0, length_scale_bounds="fixed")</code> kernel).</p> <p><img src="/images/digitalGarden/linearKernelGPR.png" alt="Linear kernel GPR" title="Linear kernel GPR"></p> <p><img src="/images/digitalGarden/coolGPReg.png" alt="Cool GP Regression" title="Cool GP Regression"></p> <p>But also, the argmax sample from the posterior predictive corresponds to the kernel ridge regression solution,</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mover accent="true"><mi>f</mi><mo>^</mo></mover><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>K</mi><mo stretchy="false">(</mo><mi>x</mi><mo separator="true">,</mo><mi>X</mi><mo stretchy="false">)</mo><mo stretchy="false">(</mo><mi>K</mi><mo stretchy="false">(</mo><mi>X</mi><mo separator="true">,</mo><mi>X</mi><mo stretchy="false">)</mo><mo>−</mo><msubsup><mi>σ</mi><mi>n</mi><mn>2</mn></msubsup><msub><mi>I</mi><mi>d</mi></msub><msup><mo stretchy="false">)</mo><mrow><mo>−</mo><mn>1</mn></mrow></msup><mi>Y</mi><mo separator="true">,</mo></mrow><annotation encoding="application/x-tex">\hat{f}(x) = K(x, X) (K(X, X) - \sigma_n^2 I_{d} )^{-1} Y,</annotation></semantics></math></span><!----></div> <p>where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>σ</mi><mi>n</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\sigma_n^2</annotation></semantics></math></span><!----></span> is your ridge penalty / noise. This is just the mean solution in the plot above, so if you don’t care about the uncertainty quantification just do this ^.</p> <p>TODO: Maybe an example with a linear kernel?</p><!----><!----></div></div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_1vrthg1 = {
						base: new URL("..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.C6bxIzgs.js"),
						import("../_app/immutable/entry/app.D3uzO-Do.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 12],
							data: [null,null],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
