<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="../favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,200..800;1,6..72,200..800&family=Roboto+Serif:ital,opsz,wght@0,8..144,100..900;1,8..144,100..900&family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
		
		<link href="../_app/immutable/assets/0.uY5HYXPu.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.DI11qAQK.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B2-O15i9.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BuAUvOro.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B_P4XA_-.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.D0zaimvy.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D8__v_G3.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/hqIbxRWK.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DKJMLC42.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DAMHlmQC.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BIEbWpQy.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.D_zbH_5b.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DhGDfn95.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/7.DbIwfNwi.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BntcpoEZ.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/wXsJu9Ig.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BLKplkQ7.js">
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><div class="max-w-[100ch] mx-auto px-4 mb-10"><nav class="flex flex-col md:flex-row md:items-center md:justify-between py-4 border-b border-olive mb-3"><a href="/" class="text-olive font-bold text-2xl md:text-2xl no-underline hover:underline mb-4 md:mb-0">Berkan</a> <div class="flex flex-col md:flex-row md:space-x-6 space-y-4 md:space-y-0"><a href="/blog" class="text-olive no-underline hover:underline">Blog</a> <a href="/projects" class="text-olive no-underline hover:underline">Projects</a> <a href="/publications" class="text-olive no-underline hover:underline">Publications</a> <a href="/resources" class="text-olive no-underline hover:underline">Resources</a></div></nav> <div><!----><p><em><strong>WARNING:</strong> This post assumes you have a basic understanding of neural networks and specifically Recurrent Neural Networks (RNNs) and it assumes that you have read part 1.</em></p> <p>Now we have a solid understading of the basics of attention mechanisms, I‚Äôm going to dive right into transformers. This post will be largely based on the original Transformers paper, <a href="https://arxiv.org/pdf/1706.03762.pdf" rel="nofollow">Attention Is All You Need</a>, but will also include additional intuitive explanation, technical details, and important context.</p> <p>I have not yet implemented this paper, but I found this <a href="https://github.com/hyunwoongko/transformer" rel="nofollow">good PyTorch implementation</a> online. I might come back to this post and add my own implementation üôÉ.</p> <p>I want to start by giving relevant background to the problem this paper aims to address.</p> <h1>Transduction in Machine Learning</h1> <p>Transfomers were originally created as transduction models in NLP. There are <a href="https://machinelearningmastery.com/transduction-in-machine-learning/" rel="nofollow">several definitions</a> of transduction models, but the definition that seems most in line with how the paper uses it is ‚Äù[learning to convert one string into another](Learning to Transduce with Unbounded Memory)‚Äú. Neural machine translation (explained in the <a href="https://berkan.xyz/posts/2022/01/transformersPart1/" rel="nofollow">last post</a>) is one of many problems that a transduction model can be used for.</p> <h1>Problems with Previous Transduction Models</h1> <p>Before 2017, when Transformers were introduced, the state of the art (SOTA) in transduction problems were mostly <a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="nofollow">LSTMs</a>, GRUs, and their variants. The problems with these models is they are recursive in nature. Recurrent models require the previous hidden state in order to compute the current hidden state, meaning they are sequential. This reduces computational efficiency because it prevents parallelization within training examples, which is especially important for training with longer input sequence lengths (training on longer articles and documents).</p> <p>There is a body of existing literature which achieves parallelization though the use of convolutional neural networks (CNNs), yet these models still struggle at learning dependencies between words that are far apart, because the number of operations required to relate words that are far apart increases with their distance. Transformers are the first transduction architecture that achieves parallelization within training examples (without convolutions or recurrence, by using attention for everything) and maintains a constant number of operations to relate words of any distance. As we will soon see, this allows transformers to achieve SOTA performance in transduction tasks with less training time than previous models.</p> <h1>Transformers Architecture Explained</h1> <p>Now for the fun part! I‚Äôm going to explain transformers by going step-by-step through a forwards pass. I will frequently reference this diagram from the paper:</p> <p><img src="/images/transformers2/transformerArchitecture.png" alt="Transformer Architecture" title="Transformer Architecture"></p> <p>Additionally, I have created my own diagram which explains things a bit more throughly. The diagram is quite big, so I recommend going to <a href="https://berkan.xyz/files/transformersDiagram.pdf" rel="nofollow">this link</a> to see the full pdf or opening the below image in a new tab. I have copy pasted bits of the diagram into the relevant sections, but you can also view the full diagram here:</p> <p><img src="/images/transformers2/transformersDiagram.png" alt="Transformer Diagram" title="Transformer Diagram"></p> <p>The model resembles the encoder-decoder structure of previous transduction models (see part 1 for details).</p> <h1>Mathematical Notation</h1> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="bold">x</mtext><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\textbf{x} \in \mathbb{R}^d</annotation></semantics></math></span><!----></span> will denote a column vector comprising of <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><msub><mi>x</mi><mn>2</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi>x</mi><mi>d</mi></msub></mrow><annotation encoding="application/x-tex">x_1, x_2, \ldots, x_d</annotation></semantics></math></span><!----></span>. I will often use a data matrix denoted as <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>√ó</mo><mi>d</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X \in \mathbb{R}^{n \times d}</annotation></semantics></math></span><!----></span>, where the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>th row corresponds to the row vector <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mtext mathvariant="bold">x</mtext><mi>i</mi><mi mathvariant="normal">‚ä§</mi></msubsup><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mi>d</mi></msup></mrow><annotation encoding="application/x-tex">\textbf{x}_i^\top \in \mathbb{R}^d</annotation></semantics></math></span><!----></span>. Therefore, by writing <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">x</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{x}_i</annotation></semantics></math></span><!----></span> I am refering to a column vector of the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>th row of <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi></mrow><annotation encoding="application/x-tex">X</annotation></semantics></math></span><!----></span>.</p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>X</mi><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>√ó</mo><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">X \in \mathbb{R}^{n \times C}</annotation></semantics></math></span><!----></span> and the corresponding <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">x</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{x}_i</annotation></semantics></math></span><!----></span> vectors will represent the input words, <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>√ó</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">H \in \mathbb{R}^{n \times d_{\text{model}}}</annotation></semantics></math></span><!----></span> and the corresponding <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">h</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{h}_i</annotation></semantics></math></span><!----></span> vectors will represent the hidden state, and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">y</mtext><mi>i</mi></msub><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mi>C</mi></msup></mrow><annotation encoding="application/x-tex">\textbf{y}_i \in \mathbb{R}^C</annotation></semantics></math></span><!----></span> will represent the output logits for the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>th output word.</p> <h1>Pre Encoder</h1> <blockquote style="background-color: #ecfeec; border-color: #528852; font-style: inherit;">I'm going to use these green blockquotes to show a running example for the embedding and unembedding of translating the sentence "Bobby likes blackberries." to German, "Bobby mag Brombeeren." <p>The input to our model is the text ‚ÄúBobby likes blackberries.‚Äù</p></blockquote> <p><img src="/images/transformers2/transformersDiagramPreEncoder.png" alt="Pre Encoder" title="Pre Encoder"></p> <h2>Tokenization</h2> <p>The first challenge is to convert the input string to a meaningful representation that the model can work with. The cannonical way to do this is to assign one hot vectors to your input corpus. The only question is what unit of your input vocabulary should you assign one hot vectors to? Should each character have it‚Äôs own vector? Should each word have it‚Äôs own vector?</p> <p>The original transformers paper, along with many other modern implementations, uses <a href="https://arxiv.org/pdf/1508.07909.pdf" rel="nofollow">byte-pair encoding</a> (BPE) to create subword tokens that can be assigned to one hot vectors (37000 tokens for English to German in the paper). It allows for common words to have their own representation, but breaks rare words into multiple subtokens in order to include all words in the corpus without an incredibly large vocabulary. Here is how BPE works (visit <a href="https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0" rel="nofollow">this page</a> for a more detailed walkthrough of this algorithm):</p> <ol><li>Split up the entire corpus into characters and add a special start of sentence token &lt;SOS>, end of word token &lt;EOW>, and end of sentence token &lt;EOS>. This is the initial token list.</li> <li>Count all token pairs and replace the individual tokens of the most common pair with the pair itself.</li> <li>Repeat step 2 until the desired vocabulary size, <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><!----></span>, is reached.</li></ol> <p>Now that we have split up the input sentence into meaningful tokens, we can create one hot vectors from these tokens. I will be using the following notation to represent one hot vectors: <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">e</mtext><mi>i</mi></msub><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mi>C</mi></msup></mrow><annotation encoding="application/x-tex">\textbf{e}_i \in \mathbb{R}^C</annotation></semantics></math></span><!----></span>, where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">e</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{e}_i</annotation></semantics></math></span><!----></span> is the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>th standard basis column vector. In other words, <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">e</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{e}_i</annotation></semantics></math></span><!----></span> represents a one hot vector where the value corresponding to the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>th dimension is 1 and the rest are 0.</p> <blockquote style="background-color: #ecfeec; border-color: #528852; font-style: inherit;">After tokenization, our input sentence "Bobby likes blackberries." is turned into the following list of tokens ["&lt;SOS>", "Bobby&lt;EOW>", "likes&lt;EOW>", "black", "berries&lt;EOW>", "&lt;EOS>"] using BPE with vocabulary size <!----><math><mrow><mi>C</mi><mo>=</mo></mrow><mrow><mn>37000</mn></mrow></math><!----> <!--[!--><!--]--><!---->. Now we can vectorize this list to get <!----><math><mrow><mrow><mo fence="true" form="prefix">[</mo><mtable columnalign="center"><mtr><mtd style="padding-left:0em;padding-right:0em;"><msubsup><mtext>ùêû</mtext><mn>41</mn><mi>‚ä§</mi></msubsup></mtd></mtr><mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mi>‚ãÆ</mi><mspace width="0pt" height="14.944pt"></mspace></mrow></mtd></mtr></mtable><mo fence="true" form="postfix">]</mo></mrow><mo>=</mo></mrow><mrow><mrow><mo fence="true" form="prefix">[</mo><mtable columnalign="center"><mtr><mtd style="padding-left:0em;padding-right:0em;"><msubsup><mtext>ùê±</mtext><mn>1</mn><mi>‚ä§</mi></msubsup></mtd></mtr><mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mi>‚ãÆ</mi><mspace width="0pt" height="14.944pt"></mspace></mrow></mtd></mtr></mtable><mo fence="true" form="postfix">]</mo></mrow><mo>=</mo></mrow><mrow><mi>X</mi><mo>‚àà</mo></mrow><mrow><msup><mi>‚Ñù</mi><mrow><mi>n</mi><mo>√ó</mo><mi>C</mi></mrow></msup></mrow></math><!----> <!--[!--><!--]--><!---->, where <!----><math><msub><mtext>ùê±</mtext><mi>i</mi></msub></math><!----> <!--[!--><!--]--><!----> is the one hot vector for the <!----><math><mi>i</mi></math><!----> <!--[!--><!--]--><!---->th input word and $n$ is called the context size. More details on the value of $n$ can be found in the decoder section. <p>(Please note that the output tokens are from BPE and your one hot vectors depend on your input and I‚Äôm just arbitrarily choosing plausible values.)</p></blockquote> <h2>Word Embeddings</h2> <p>But these one hot input vectors are still very high dimensional. Luckily, we can embbed these vectors into a lower dimensional vector space that is easier for the model to work with. To do this, the original transformers paper uses weight sharing of the input embeddings, output embeddings, and unembedding to get the logits, similar to what was done by <a href="https://arxiv.org/pdf/1608.05859.pdf" rel="nofollow">Press et al.</a> (see the paper for more details, I will just give a brief explanation).</p> <p>A word embedding matrix <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>√ó</mo><mi>C</mi></mrow></msup></mrow><annotation encoding="application/x-tex">U \in \mathbb{R}^{d_{\text{model}} \times C}</annotation></semantics></math></span><!----></span> (<span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mtext>model</mtext></msub></mrow><annotation encoding="application/x-tex">d_{\text{model}}</annotation></semantics></math></span><!----></span> is just the dimension of many word representations in the model, it is <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>512</mn></mrow><annotation encoding="application/x-tex">512</annotation></semantics></math></span><!----></span> in the original transformers paper) is used to convert <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">x</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{x}_i</annotation></semantics></math></span><!----></span> into a lower dimensional vector <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi><msub><mtext mathvariant="bold">x</mtext><mi>i</mi></msub><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><msub><mi>d</mi><mtext>model</mtext></msub></msup></mrow><annotation encoding="application/x-tex">U \textbf{x}_i \in \mathbb{R}^{d_{\text{model}}}</annotation></semantics></math></span><!----></span> that matches the model size and represents the properties of the input word. We will use the same matrix <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><!----></span> to unembed our hidden state to get the logits (see post decoder section). We train <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><!----></span> along with the rest of our network (no pretrained word embeddings used).</p> <blockquote style="background-color: #ecfeec; border-color: #528852; font-style: inherit;">Now we can take our list <!----><math><mrow><mrow><mo fence="true" form="prefix">[</mo><mtable columnalign="center"><mtr><mtd style="padding-left:0em;padding-right:0em;"><msubsup><mtext>ùê±</mtext><mn>1</mn><mi>‚ä§</mi></msubsup></mtd></mtr><mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mi>‚ãÆ</mi><mspace width="0pt" height="14.944pt"></mspace></mrow></mtd></mtr></mtable><mo fence="true" form="postfix">]</mo></mrow><mo>=</mo></mrow><mrow><mi>X</mi></mrow></math><!----> <!--[!--><!--]--><!----> of token one hot vectors, and compute the word embeddings <!----><math><mrow><mrow><mo fence="true" form="prefix">[</mo><mtable columnalign="center"><mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mo form="prefix" stretchy="false">(</mo><mi>U</mi><msub><mtext>ùê±</mtext><mn>1</mn></msub><msup><mo form="postfix" stretchy="false">)</mo><mi>‚ä§</mi></msup></mrow></mtd></mtr><mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mi>‚ãÆ</mi><mspace width="0pt" height="14.944pt"></mspace></mrow></mtd></mtr></mtable><mo fence="true" form="postfix">]</mo></mrow><mo>=</mo></mrow><mrow><mi>X</mi><msup><mi>U</mi><mi>‚ä§</mi></msup><mo>‚àà</mo></mrow><mrow><msup><mi>‚Ñù</mi><mrow><mi>n</mi><mo>√ó</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow></math><!----> <!--[!--><!--]--><!---->.</blockquote> <h2>Positional Encodings</h2> <p>Because transformers don‚Äôt have any sort of recurrence or convolutions, the model has no way of understanding the order of the inputs. The model needs some form of positional encoding of the input tokens. In the original transformers paper, these encodings were explicitly given and not learned (empirical evidence suggests learned embeddings didn‚Äôt perform better in general and even performed worse for input sequences longer than the ones in the training data).</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi>sin</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">/</mi><msup><mn>1000</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE_{(pos, 2i)} = \sin(pos/1000^{2i/d_{\text{model}}})</annotation></semantics></math></span><!----></div> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mo separator="true">,</mo><mn>2</mn><mi>i</mi><mo>+</mo><mn>1</mn><mo stretchy="false">)</mo></mrow></msub><mo>=</mo><mi>cos</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mi>p</mi><mi>o</mi><mi>s</mi><mi mathvariant="normal">/</mi><msup><mn>1000</mn><mrow><mn>2</mn><mi>i</mi><mi mathvariant="normal">/</mi><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">PE_{(pos, 2i + 1)} = \cos(pos/1000^{2i/d_{\text{model}}})</annotation></semantics></math></span><!----></div> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>P</mi><mi>E</mi><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mn>1</mn><mi mathvariant="normal">/</mi><msup><mn>1000</mn><mrow><mn>1</mn><mi mathvariant="normal">/</mi><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mn>1</mn><mi mathvariant="normal">/</mi><msup><mn>1000</mn><mrow><mn>2</mn><mi mathvariant="normal">/</mi><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">‚ãØ</mo></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>cos</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mn>2</mn><mi mathvariant="normal">/</mi><msup><mn>1000</mn><mrow><mn>1</mn><mi mathvariant="normal">/</mi><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi>sin</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mn>2</mn><mi mathvariant="normal">/</mi><msup><mn>1000</mn><mrow><mn>2</mn><mi mathvariant="normal">/</mi><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">‚ãÆ</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">‚ã±</mo></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>√ó</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">PE = \begin{bmatrix}
  \cos(1/1000^{1/d_{\text{model}}}) &amp; \sin(1/1000^{2/d_{\text{model}}}) &amp; \cdots\\
  \cos(2/1000^{1/d_{\text{model}}}) &amp; \sin(2/1000^{2/d_{\text{model}}}) &amp; \\
  \vdots &amp;  &amp; \ddots
\end{bmatrix} \in \mathbb{R}^{n \times d_{\text{model}}}</annotation></semantics></math></span><!----></div> <p>Where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">pos</annotation></semantics></math></span><!----></span> is the position of the token and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span> is a dimension in that token‚Äôs embedding. This is what the encodings partially look like (image taken from the <a href="http://jalammar.github.io/illustrated-transformer/" rel="nofollow">illustrated transformer</a>).</p> <p><img src="/images/transformers2/attention-is-all-you-need-positional-encoding.png" alt="Positional Encodings" title="Positional Encodings"></p> <p>The authors hypothesized that these positional encodings would allow the model to easily learn to attend by relative positions, since for any fixed offset <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><!----></span>, <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi><mo>+</mo><mi>k</mi></mrow></msub></mrow><annotation encoding="application/x-tex">PE_{pos+k}</annotation></semantics></math></span><!----></span> can be represented as a linear function of <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>P</mi><msub><mi>E</mi><mrow><mi>p</mi><mi>o</mi><mi>s</mi></mrow></msub></mrow><annotation encoding="application/x-tex">PE_{pos}</annotation></semantics></math></span><!----></span>.</p> <p>These positional encodings get summed with the original embeddings to create the input into the transformer encoder. I‚Äôm going to use <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>√ó</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">H \in \mathbb{R}^{n \times d_{\text{model}}}</annotation></semantics></math></span><!----></span> to indicate hidden state of the model (note that the dimension of the hidden state never changes in any of the transformer layers).</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>H</mi><mo>=</mo><mi>X</mi><msup><mi>U</mi><mi mathvariant="normal">‚ä§</mi></msup><mo>+</mo><mi>P</mi><mi>E</mi></mrow><annotation encoding="application/x-tex">H = X U^\top + PE</annotation></semantics></math></span><!----></div> <blockquote style="background-color: #ecfeec; border-color: #528852; font-style: inherit;">We can represent our input sentence as <!----><math><mrow><mi>H</mi><mo>=</mo></mrow><mrow><mi>X</mi><msup><mi>U</mi><mi>‚ä§</mi></msup><mo>+</mo></mrow><mrow><mi>P</mi><mi>E</mi><mo>‚àà</mo></mrow><mrow><msup><mi>‚Ñù</mi><mrow><mi>n</mi><mo>√ó</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow></math><!----> <!--[!--><!--]--><!---->, where we have one column vector that represents for each input token and its position.</blockquote> <h1>Encoder</h1> <p>Now we are finally ready to dive into the encoder part of the model. In the original paper, the encoder consists of a stack of 6 transformer layers, each with two sublayers. The first sublayer is multi-head attention, and the second is just a multilayer perceptron (MLP). Additionally, there is a residual connection between each of these layers, and two layer normalizations, one after each sublayer.</p> <p><img src="/images/transformers2/transformersDiagramEncoder.png" alt="Encoder" title="Encoder"></p> <h2>Residual Stream</h2> <p>A nice way to think about these models, inspired by <a href="https://transformer-circuits.pub/2021/framework/index.html#residual-comms" rel="nofollow">Elhage et al.</a>, is that we have a stream of data that is being added to by the multi-head attention and MLPs and is normalized by the layer normalizations. This stream is almost entirely linear, and it serves as an important communications channel throughout the model. The dimensions of this stream are also never changed throughout the entire model.</p> <p><img src="/images/transformers2/anthropicTransformer.png" alt="Residual Stream in Transformer" title="Residual Stream in Transformer"></p> <h2>Multi-Head Attention</h2> <p>We can think of the function of multi-head attention as <a href="https://transformer-circuits.pub/2021/framework/index.html#residual-comms" rel="nofollow">moving information</a> in the residual stream and applying some weight matrices. Let‚Äôs start with a single attention head:</p> <h3>Scaled Dot Product Attention for One Attention Head</h3> <p>The transformers architecture implements scaled dot product attention. This is in contrast to additive attention, which was described in the <a href="https://berkan.xyz/posts/2022/01/transformersPart1/" rel="nofollow">last post</a>. In additive attention, we have an entire alignment model (a small MLP) that computes the logits that get fed into the softmax to determine the weighting of the values. In dot product attention, we replace that alignment model with dot products and linear transformations!</p> <p>If you read the original transformers paper, you will see scaled dot product attention written in the most general form as:</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi mathvariant="normal">‚ä§</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\text{Attention}(Q, K, V) = \text{softmax} \big( \frac{QK^\top}{\sqrt{d_k}} \big) V</annotation></semantics></math></span><!----></div> <p>Let‚Äôs ignore what the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><!----></span>, <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><!----></span>, and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><!----></span> matricies represent for the moment. Initially, this notation was a bit confusing for me at first because, technically the softmax function is a vector function. The key insight is that the softmax acts on the rows of the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><msup><mi>K</mi><mi mathvariant="normal">‚ä§</mi></msup></mrow><annotation encoding="application/x-tex">Q K^\top</annotation></semantics></math></span><!----></span> matrix. Things are more clear when they are written as follows:</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>softmax</mtext><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">(</mo><mfrac><mrow><msubsup><mtext mathvariant="bold">q</mtext><mi>i</mi><mi mathvariant="normal">‚ä§</mi></msubsup><msup><mi>K</mi><mi mathvariant="normal">‚ä§</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo fence="false" stretchy="true" minsize="1.2em" maxsize="1.2em">)</mo><mi>V</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">‚ãÆ</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Attention}(Q, K, V) = \begin{bmatrix}
  \text{softmax}\big(\frac{\textbf{q}_i^\top K^\top}{\sqrt{d_k}}\big) V \\
  \vdots
\end{bmatrix} </annotation></semantics></math></span><!----></div> <p>Essentially, we are just taking some convex combination of the rows of <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><!----></span> in each row of our attention matrix.</p> <h3>Relation to Additive Attention</h3> <p>As a refresher, additive attention from the previous post looked like this:</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext mathvariant="bold">c</mtext><mi>i</mi></msub><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>x</mi></msub></munderover><msub><mi>Œ±</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mtext mathvariant="bold">h</mtext><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{c}_i = \sum_{j=1}^{T_x} \alpha_{ij} \textbf{h}_j</annotation></semantics></math></span><!----></div> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Œ±</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">e</mtext><mi>i</mi></msub><msub><mo stretchy="false">)</mo><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_{ij} = \text{softmax}(\textbf{e}_{i})_j</annotation></semantics></math></span><!----></div> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext mathvariant="bold">e</mtext><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>a</mi><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">s</mtext><mrow><mi>i</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mtext mathvariant="bold">h</mtext><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\textbf{e}_{ij} = a(\textbf{s}_{i-1}, \textbf{h}_j)</annotation></semantics></math></span><!----></div> <p>Where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">c</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{c}_i</annotation></semantics></math></span><!----></span> was the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>th context vector, <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><!----></span> was the alignment model, <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">s</mtext><mrow><mi>i</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\textbf{s}_{i-1}</annotation></semantics></math></span><!----></span> was the previous hidden state of the decoder, and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="bold">h</mtext><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{h}_j</annotation></semantics></math></span><!----></span> was the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><!----></span>th output from the bidirectional RNN encoder. We can summarize the equations above as follows (note that the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>T</mi><mi>x</mi></msub><mo>=</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">T_x = n</annotation></semantics></math></span><!----></span> from the previous post and just represents the input length):</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext mathvariant="bold">c</mtext><mi>i</mi></msub><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><mtext>softmax</mtext><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">s</mtext><mrow><mi>i</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi>H</mi><mo stretchy="false">)</mo><msub><mo stretchy="false">)</mo><mi>j</mi></msub><msub><mtext mathvariant="bold">h</mtext><mi>j</mi></msub><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">s</mtext><mrow><mi>i</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi>H</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">\textbf{c}_i = \sum_{j=1}^{n} \text{softmax}(a(\textbf{s}_{i-1}, H))_j \textbf{h}_j = \text{softmax}(a(\textbf{s}_{i-1}, H)) H</annotation></semantics></math></span><!----></div> <p>If we take <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi><mo>=</mo><msub><mtext mathvariant="bold">s</mtext><mrow><mi>i</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">Q = \textbf{s}_{i-1}</annotation></semantics></math></span><!----></span> and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi><mo>=</mo><mi>V</mi><mo>=</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">K = V = H</annotation></semantics></math></span><!----></span> we can write:</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext mathvariant="bold">c</mtext><mi>i</mi></msub><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">s</mtext><mrow><mi>i</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><mi>H</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi>H</mi><mo>=</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mi>a</mi><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">q</mtext><mi>i</mi></msub><mo separator="true">,</mo><mi>K</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\textbf{c}_i = \text{softmax}(a(\textbf{s}_{i-1}, H)) H = \text{softmax}(a(\textbf{q}_i, K)) V</annotation></semantics></math></span><!----></div> <p>In principle, all both of these attention mechanisms do is take convex combinations of the rows of some value matrix! The main difference is just how the convex combination weights are calcuated. Additive attention uses a small MLP alignment model, and dot product attention uses dot products. In fact, if <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi><mo stretchy="false">(</mo><mtext mathvariant="bold">x</mtext><mo separator="true">,</mo><mi>Y</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><msup><mtext mathvariant="bold">x</mtext><mi mathvariant="normal">‚ä§</mi></msup><msup><mi>Y</mi><mi mathvariant="normal">‚ä§</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mrow><annotation encoding="application/x-tex">a(\textbf{x}, Y) = \frac{\textbf{x}^\top Y^\top}{\sqrt{d_k}}</annotation></semantics></math></span><!----></span> the formula above exactly describes scaled dot product attention for one token.  The transformer paper claims that while these are similar in theoretical complexity, dot-product attention is much faster and more space efficient on modern hardware. I find it incredible how we can basically just replace a small neural network with a dot product and maintain good performance ü§Ø.</p> <h3>Why Scaled Dot-Product Attention?</h3> <p>You might be wondering why this is called <strong>scaled</strong> dot-product attention. That is because of the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mfrac><mn>1</mn><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac></mrow><annotation encoding="application/x-tex">\frac{1}{\sqrt{d_k}}</annotation></semantics></math></span><!----></span> term in the softmax. The paper claims this is because additive attention outperforms dot product attention for large values of <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><!----></span> without the scaling. They suspect this is becuase the dot products grow large in magnitude, pushing the softmax function into regions of <a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="nofollow">extremely small gradients</a>, which slows gradient descent. Let‚Äôs disect why this is true, why do the dot products grow large in magnitude and why does this push the softmax function into regions of small gradients?</p> <p>We can think of a dot product between two vectors of dimension <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><!----></span> as <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="bold">v</mtext><mo>‚ãÖ</mo><mtext mathvariant="bold">w</mtext><mo>=</mo><msub><mo>‚àë</mo><mi>i</mi></msub><msub><mtext mathvariant="bold">v</mtext><mi>i</mi></msub><msub><mtext mathvariant="bold">w</mtext><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\textbf{v} \cdot \textbf{w} = \sum_i \textbf{v}_i \textbf{w}_i</annotation></semantics></math></span><!----></span>. If each entry in these vectors is a random variable with mean 0 and variance 1, then because variances add, the dot product will have mean 0 and variance <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><!----></span>. This is of course an oversimplification because the entires of our queries and keys will not be random variables with mean 0 and variance 1, but it illustrates the fact that as <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><!----></span> grows, so does the variance of our dot product. Now how does this impact the gradient of the softmax function (technically gradients are only defined for scalar functions, so from now on I will refer to the <a href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/" rel="nofollow">derivative of the softmax function</a>). Here is the derivative of the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>th output with respect to the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><!----></span>th input.</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>softmax</mtext><mo stretchy="false">(</mo><mtext mathvariant="bold">v</mtext><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mo>=</mo><mfrac><msup><mi>e</mi><msub><mtext mathvariant="bold">v</mtext><mi>i</mi></msub></msup><mrow><munderover><mo>‚àë</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msup><mi>e</mi><msub><mtext mathvariant="bold">v</mtext><mi>k</mi></msub></msup></mrow></mfrac><mo separator="true">,</mo><mtext>¬†</mtext><mtext mathvariant="bold">v</mtext><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mi>N</mi></msup></mrow><annotation encoding="application/x-tex">\text{softmax}(\textbf{v})_i = \frac{e^{\textbf{v}_i}}{\sum_{k=1}^N e^{\textbf{v}_k}}, \ \textbf{v} \in \mathbb{R}^N</annotation></semantics></math></span><!----></div> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mfrac><mrow><mi mathvariant="normal">‚àÇ</mi><mtext>softmax</mtext><mo stretchy="false">(</mo><mtext mathvariant="bold">v</mtext><msub><mo stretchy="false">)</mo><mi>i</mi></msub></mrow><mrow><mi mathvariant="normal">‚àÇ</mi><msub><mtext mathvariant="bold">v</mtext><mi>j</mi></msub></mrow></mfrac><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>softmax</mtext><mo stretchy="false">(</mo><mtext mathvariant="bold">v</mtext><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mo stretchy="false">(</mo><mn>1</mn><mo>‚àí</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mtext mathvariant="bold">v</mtext><msub><mo stretchy="false">)</mo><mi>j</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if¬†</mtext><mi>i</mi><mo>=</mo><mi>j</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mtext>softmax</mtext><mo stretchy="false">(</mo><mtext mathvariant="bold">v</mtext><msub><mo stretchy="false">)</mo><mi>i</mi></msub><mtext>softmax</mtext><mo stretchy="false">(</mo><mtext mathvariant="bold">v</mtext><msub><mo stretchy="false">)</mo><mi>j</mi></msub></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if¬†</mtext><mi>i</mi><mo mathvariant="normal">‚â†</mo><mi>j</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">  \frac{\partial \text{softmax}(\textbf{v})_i}{\partial \textbf{v}_j} = \begin{cases} 
    \text{softmax}(\textbf{v})_i (1 - \text{softmax}(\textbf{v})_j) &amp; \text{if } i = j \\
    - \text{softmax}(\textbf{v})_i \text{softmax}(\textbf{v})_j &amp; \text{if }  i \neq j 
  \end{cases}</annotation></semantics></math></span><!----></div> <p>Due to the exponent, when there is a large term in the input vector to the softmax, it ends up dominating the output probability distribution. Therefore, if we have large dot products from before, the input to the softmax is going to have some terms that are very large. In both cases for the derivative, it is likely that either <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>softmax</mtext><mo stretchy="false">(</mo><mtext mathvariant="bold">v</mtext><msub><mo stretchy="false">)</mo><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">\text{softmax}(\textbf{v})_i</annotation></semantics></math></span><!----></span> or <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>softmax</mtext><mo stretchy="false">(</mo><mtext mathvariant="bold">v</mtext><msub><mo stretchy="false">)</mo><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\text{softmax}(\textbf{v})_j</annotation></semantics></math></span><!----></span> will be small, making the overall derivative also very small.</p> <p>However, by adding the scaling term that is proportional to <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{d_k}</annotation></semantics></math></span><!----></span>, where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">d_k</annotation></semantics></math></span><!----></span> is the dimension of the dotted vectors, we make the values in the input vector to the softmax less extreme, and therefore make it less likely that the overall derivative is very small.</p> <h3>Multi-Head Attention</h3> <p>Instead of just using one attention head as in the original attention paper, the authors found it beneficial to linearly project the queries, keys, and values <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><!----></span> times to a lower dimensional space, run attention on each, and aggregate the results.</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>MultiHead</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>Concat</mtext><mo stretchy="false">(</mo><msub><mtext>head</mtext><mn>1</mn></msub><mo separator="true">,</mo><msub><mtext>head</mtext><mn>2</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><msub><mtext>head</mtext><mi>h</mi></msub><mo stretchy="false">)</mo><msup><mi>W</mi><mi>O</mi></msup></mrow><annotation encoding="application/x-tex">\text{MultiHead}(Q, K, V) = \text{Concat}(\text{head}_1, \text{head}_2, \ldots \text{head}_h) W^O</annotation></semantics></math></span><!----></div> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mtext>head</mtext><mi>i</mi></msub><mo>=</mo><mtext>Attention</mtext><mo stretchy="false">(</mo><mi>Q</mi><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo separator="true">,</mo><mi>K</mi><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo separator="true">,</mo><mi>V</mi><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\text{head}_i = \text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)</annotation></semantics></math></span><!----></div> <p>Where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>Q</mi></msubsup><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>√ó</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W_i^Q \in \mathbb{R}^{d_{\text{model}} \times d_k}</annotation></semantics></math></span><!----></span>, <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>K</mi></msubsup><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>√ó</mo><msub><mi>d</mi><mi>k</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W_i^K \in \mathbb{R}^{d_{\text{model}} \times d_k}</annotation></semantics></math></span><!----></span>, <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>W</mi><mi>i</mi><mi>V</mi></msubsup><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><msub><mi>d</mi><mtext>model</mtext></msub><mo>√ó</mo><msub><mi>d</mi><mi>v</mi></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W_i^V \in \mathbb{R}^{d_{\text{model}} \times d_v}</annotation></semantics></math></span><!----></span>, and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>W</mi><mi>O</mi></msup><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>h</mi><msub><mi>d</mi><mi>v</mi></msub><mo>√ó</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">W^O \in \mathbb{R}^{h d_v \times d_{\text{model}}}</annotation></semantics></math></span><!----></span>. Additionally, they set <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>=</mo><mn>8</mn></mrow><annotation encoding="application/x-tex">h=8</annotation></semantics></math></span><!----></span> and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>d</mi><mi>k</mi></msub><mo>=</mo><msub><mi>d</mi><mi>v</mi></msub><mo>=</mo><msub><mi>d</mi><mtext>model</mtext></msub><mi mathvariant="normal">/</mi><mi>h</mi><mo>=</mo><mn>64</mn></mrow><annotation encoding="application/x-tex">d_k = d_v = d_{\text{model}}/h = 64</annotation></semantics></math></span><!----></span>. The computational cost of this is similar to if they had just used a single head attention with full dimensionality.</p> <h3>Multi-Head Self-Attention in the Encoder</h3> <p>The way this multi-head attention plays out in the encoder is as self-attention, meaning that the <a href="https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms" rel="nofollow">queries, keys, and values</a> all are equal to the hidden state <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><!----></span>.</p> <p>One reasonable question is, why do we even need multi-head self-attention? One thing attention gets you is the ability to mix information between tokens. This is the only part of the architecture that has this capability.</p> <h2>Layer Normalization</h2> <p><a href="https://arxiv.org/pdf/1607.06450.pdf" rel="nofollow">Layer normalization</a> is a commonly used method to reduce training time by normalizing hidden states within a single training example. Nowadays, it seems <a href="https://paperswithcode.com/method/layer-normalization" rel="nofollow">more commonly used</a> than <a href="https://arxiv.org/pdf/1502.03167.pdf" rel="nofollow">batch normalization</a> because it is able to be used for online learning tasks or places where it is impossible to have large mini-batches, and is difficult to apply to RNNs. Here is a <a href="https://leimao.github.io/blog/Layer-Normalization/" rel="nofollow">mathematical explanation</a> of how it works in transformers:</p> <p>First we compute the mean and variance of each of the n tokens of the our training (or test) sample:</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>Œº</mi><mi>t</mi></msub><mo>=</mo><mfrac><mn>1</mn><msub><mi>d</mi><mtext>model</mtext></msub></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>d</mi><mtext>model</mtext></msub></munderover><msub><mtext mathvariant="bold">h</mtext><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\mu_{t} = \frac{1}{d_{\text{model}}} \sum_{i=1}^{d_{\text{model}}} \textbf{h}_{t, i}</annotation></semantics></math></span><!----></div> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>œÉ</mi><mi>t</mi></msub><mo>=</mo><msqrt><mrow><mfrac><mn>1</mn><msub><mi>d</mi><mtext>model</mtext></msub></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>d</mi><mtext>model</mtext></msub></munderover><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">h</mtext><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub><mo>‚àí</mo><msub><mi>Œº</mi><mi>t</mi></msub><msup><mo stretchy="false">)</mo><mn>2</mn></msup></mrow></msqrt></mrow><annotation encoding="application/x-tex">\sigma_{t} = \sqrt{\frac{1}{d_{\text{model}}} \sum_{i=1}^{d_{\text{model}}} (\textbf{h}_{t, i} - \mu_{t})^2}</annotation></semantics></math></span><!----></div> <p>Where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œº</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\mu_{t}</annotation></semantics></math></span><!----></span> represents the mean for token <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><!----></span> and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>H</mi><mrow><mi>t</mi><mo separator="true">,</mo><mi>i</mi></mrow></msub></mrow><annotation encoding="application/x-tex">H_{t, i}</annotation></semantics></math></span><!----></span> represents the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>th dimension of the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi></mrow><annotation encoding="application/x-tex">t</annotation></semantics></math></span><!----></span>th token.</p> <p>The rest of layer normlization was quite suprising to me. We also add <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ≥</mi><mo separator="true">,</mo><mi>Œ≤</mi><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><msub><mi>d</mi><mtext>model</mtext></msub></msup></mrow><annotation encoding="application/x-tex">\gamma, \beta \in \mathbb{R}^{d_{\text{model}}}</annotation></semantics></math></span><!----></span> terms that get learned through backprop and are applied as follows:</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>LayerNorm</mtext><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">h</mtext><mi>t</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>Œ≥</mi><mfrac><mrow><msub><mtext mathvariant="bold">h</mtext><mi>t</mi></msub><mo>‚àí</mo><msub><mi>Œº</mi><mi>t</mi></msub></mrow><mrow><msub><mi>œÉ</mi><mi>t</mi></msub><mo>+</mo><mi>œµ</mi></mrow></mfrac><mo>+</mo><mi>Œ≤</mi></mrow><annotation encoding="application/x-tex">\text{LayerNorm}(\textbf{h}_{t}) = \gamma \frac{\textbf{h}_{t} - \mu_{t}}{\sigma_{t} + \epsilon} + \beta</annotation></semantics></math></span><!----></div> <h2>MLP</h2> <p>The MLP layers of the transformer are applied identically to each token in the hidden state, and are added back into the residual stream. These layers are essentially comprised of two affine transformations with a ReLU (<a href="https://arxiv.org/pdf/1606.08415.pdf" rel="nofollow">GeLUs</a> are usually used in many modern models, but not in the original paper) in the middle. This can be written as follows:</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>MLP</mtext><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">h</mtext><mi>i</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mtext>ReLU</mtext><mo stretchy="false">(</mo><msub><mtext mathvariant="bold">h</mtext><mi>i</mi></msub><msub><mi>W</mi><mn>1</mn></msub><mo>+</mo><msub><mtext mathvariant="bold">b</mtext><mn>1</mn></msub><mo stretchy="false">)</mo><msub><mi>W</mi><mn>2</mn></msub><mo>+</mo><msub><mtext mathvariant="bold">b</mtext><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">\text{MLP}(\textbf{h}_i) = \text{ReLU}(\textbf{h}_i W_1 + \textbf{b}_1) W_2 + \textbf{b}_2</annotation></semantics></math></span><!----></div> <p>One obvious question might be, what do these MLPs do? <a href="https://transformer-circuits.pub/2022/solu/index.html" rel="nofollow">Recent work from Anthropic</a> suggests that in language models neurons in MLPs may represent certain rules or categories such as phrases related to music or base64-encoded text. From my own work, it seems that, at least in early layers, of Vision Transformers, MLP neurons often correspond to certain visual textures or patterns.</p> <h2>Layer Normalization</h2> <p>After the MLP, there is another LayerNorm, which is applied identically to the one before.</p> <h2>Final Encoder Output</h2> <p>What was described above was one encoder block. Note that the input dimensions and output dimensions of each transformer block are the same, namely <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mrow><mi>n</mi><mo>√ó</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow><annotation encoding="application/x-tex">H \in \mathbb{R}^{n \times d_{\text{model}}}</annotation></semantics></math></span><!----></span>. Therefore, we can just take the output of our first block, and feed it into our second block. This is done <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><!----></span> times. The final hidden state that comes out of the encoder blocks then becomes the key and value matrices for the decoder <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>K</mi><mi>E</mi></msub><mo separator="true">,</mo><msub><mi>V</mi><mi>E</mi></msub><mo>=</mo><mi>H</mi></mrow><annotation encoding="application/x-tex">K_E, V_E = H</annotation></semantics></math></span><!----></span>, more on this later. The encoder is only ran once.</p> <h1>Pre Decoder</h1> <p>In the original transformers paper, the decoder is used autoregressively, meaning that output translations are fed back into the model. Initially, nothing except for a start of sentence token &lt;SOS> is fed into the model. But as the model starts making predictions, the input sequence to the decoder grows. The input into the encoder is processed the same way as the input to the encoder (see the pre-encoder section).</p> <blockquote style="background-color: #ecfeec; border-color: #528852; font-style: inherit;">Our current output sentence is "Bobby mag", which is tokenized as ["&lt;SOS>", "Bobby&lt;EOW>", "mag&lt;EOW>"] using BPE with vocabulary size $C = 37000$. Now we can vectorize this list to get <!----><math><mrow><mrow><mo fence="true" form="prefix">[</mo><mtable columnalign="center"><mtr><mtd style="padding-left:0em;padding-right:0em;"><msubsup><mtext>ùêû</mtext><mn>41</mn><mi>‚ä§</mi></msubsup></mtd></mtr><mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mi>‚ãÆ</mi><mspace width="0pt" height="14.944pt"></mspace></mrow></mtd></mtr></mtable><mo fence="true" form="postfix">]</mo></mrow><mo>=</mo></mrow><mrow><mrow><mo fence="true" form="prefix">[</mo><mtable columnalign="center"><mtr><mtd style="padding-left:0em;padding-right:0em;"><msubsup><mtext>ùê±</mtext><mn>1</mn><mi>‚ä§</mi></msubsup></mtd></mtr><mtr><mtd style="padding-left:0em;padding-right:0em;"><mrow><mi>‚ãÆ</mi><mspace width="0pt" height="14.944pt"></mspace></mrow></mtd></mtr></mtable><mo fence="true" form="postfix">]</mo></mrow><mo>=</mo></mrow><mrow><mi>X</mi><mo>‚àà</mo></mrow><mrow><msup><mi>‚Ñù</mi><mrow><mi>n</mi><mo>√ó</mo><mi>C</mi></mrow></msup></mrow></math><!----> <!--[!--><!--]--><!----> , where <!----><math><msub><mtext>ùê±</mtext><mi>i</mi></msub></math><!----> <!--[!--><!--]--><!----> is the one hot vector for the $i$th input word. We again embed the words and add positional encodings as follows, <!----><math><mrow><mi>H</mi><mo>=</mo></mrow><mrow><mi>X</mi><msup><mi>U</mi><mi>‚ä§</mi></msup><mo>+</mo></mrow><mrow><mi>P</mi><mi>E</mi><mo>‚àà</mo></mrow><mrow><msup><mi>‚Ñù</mi><mrow><mi>n</mi><mo>√ó</mo><msub><mi>d</mi><mtext>model</mtext></msub></mrow></msup></mrow></math><!----> <!--[!--><!--]--><!---->. <p>(Please note that the output tokens are again from BPE and your one hot vectors depend on your input and I‚Äôm just arbitrarily choosing plausible values.)</p></blockquote> <p><img src="/images/transformers2/transformersDiagramPreDecoder.png" alt="PreDecoder" title="PreDecoder"></p> <h1>Decoder</h1> <p>The decoder is almost exactly the same as the encoder, except the attention works differently. Instead of just one multi-head self-attention block, we have a masked multi-head self-attention block and a multi-head attention block. So all in all, we have our embedding, then masked mutli-head self-attention, then layer norm, then multi-head attention, then layer norm, then an MLP, and finally one last layer norm.</p> <p><img src="/images/transformers2/transformersDiagramDecoder.png" alt="Decoder" title="Decoder"> <em>Please note:</em> There is a slight abuse of notation in the diagram above, the query, key, and value weight matrices in the masked attention and the regular attention can have different values, but are just denoted the same.</p> <h2>Masked Multi-Head Self-Attention</h2> <p>Instead of using regular multi-head self-attention, the decoder uses masked multi-head self-attention. Masked multi-head self-attention doesn‚Äôt allow for tokens to attend to later tokens. This is implemented by making out (setting to <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚àí</mo><mi mathvariant="normal">‚àû</mi></mrow><annotation encoding="application/x-tex">-\infty</annotation></semantics></math></span><!----></span>) all values in the input of the softmax which would cause attention to later tokens. This just requires us to slightly modify our equation for attention and add in a ‚Äúlook ahead mask‚Äù.</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mtext>MaskedAttention</mtext><mo stretchy="false">(</mo><mi>Q</mi><mo separator="true">,</mo><mi>K</mi><mo separator="true">,</mo><mi>V</mi><mo stretchy="false">)</mo><mo>=</mo><mtext>softmax</mtext><mo fence="false" stretchy="true" minsize="1.8em" maxsize="1.8em">(</mo><mfrac><mrow><mi>Q</mi><msup><mi>K</mi><mi mathvariant="normal">‚ä§</mi></msup></mrow><msqrt><msub><mi>d</mi><mi>k</mi></msub></msqrt></mfrac><mo>+</mo><mrow><mo fence="true">[</mo><mtable rowspacing="0.16em" columnalign="center center center center" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mi mathvariant="normal">‚àû</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">‚ãØ</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mi mathvariant="normal">‚àû</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">‚ãÆ</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mi mathvariant="normal">‚àû</mi></mrow></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mi mathvariant="normal">‚ãÆ</mi><mpadded height="0em" voffset="0em"><mspace mathbackground="black" width="0em" height="1.5em"></mspace></mpadded></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mo>‚àí</mo><mi mathvariant="normal">‚àû</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mo lspace="0em" rspace="0em">‚ãØ</mo></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>0</mn></mstyle></mtd></mtr></mtable><mo fence="true">]</mo></mrow><mo fence="false" stretchy="true" minsize="1.8em" maxsize="1.8em">)</mo><mi>V</mi></mrow><annotation encoding="application/x-tex">\text{MaskedAttention}(Q, K, V) = \text{softmax} \Big( \frac{QK^\top}{\sqrt{d_k}} + 

\begin{bmatrix}
  0 &amp; -\infty &amp; \cdots &amp; -\infty \\
  \vdots &amp; 0 &amp; -\infty &amp; \vdots \\
  0 &amp; 0 &amp; 0 &amp; -\infty \\
  0 &amp; 0 &amp; \cdots &amp; 0
\end{bmatrix} 

\Big) V </annotation></semantics></math></span><!----></div> <p>The paper claims masking is necessary ‚Äúto prevent leftward information flow in the decoder to preserve the auto-regressive property.‚Äù To be more precise, I think the word ‚Äúprevent‚Äù should be replaced with ‚Äúreduce‚Äù. The reason being that technically, even for the first attention head, the tokens of the value matrix don‚Äôt have to precisely correspond to tokens. This is because the value weight matrix applies some linear transformation to the hidden state. This transformation could mix information between tokens, such that even if it is impossible for the softmax to produce connections to later tokens, the tokens representations themselves could have information from later tokens. That being said, in practice, it seems that this still enables the transformer to preserve auto-regressive property because the model works!</p> <p>If we didn‚Äôt train in parallel, we actually wouldn‚Äôt need masking at all. <a href="https://stackoverflow.com/questions/58127059/how-to-understand-masked-multi-head-attention-in-transformer" rel="nofollow">This is because</a> we could technically do all our training recurrently, where with each token we autoregressively generate, we save the hidden state for that token, and then use it when generating the next token (more on this later).</p> <h2>Multi-Head Attention</h2> <p>This multi-head attention block is almost the same as the multi-head self-attention in the encoder, except that the key and value matrices come from the output of the encoder. Taking the keys and values from the encoder is necessary in order to incorporate information from the the input sentence into the output translation.</p> <p>Note, because the value matrix is not the current decoder hidden state <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><!----></span>, if the number of tokens in the encoder and decoder were different, we would not be able to add back into the residual stream! Therefore, in practice, a default max number of tokens, or context size, <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><!----></span> is set (to 128 for instance) for both the encoder and decoder blocks. If the sequence is not <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><!----></span> tokens long, the remaining tokens are filled in by padding &lt;PAD> tokens.</p> <h1>Post Decoder</h1> <p>Just like with the encoder, there are <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><!----></span> of decoder blocks. To get our output logits using the hidden state <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi></mrow><annotation encoding="application/x-tex">H</annotation></semantics></math></span><!----></span> passed through the transformer decoder layers, we use the embedding matrix <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>U</mi></mrow><annotation encoding="application/x-tex">U</annotation></semantics></math></span><!----></span> to unembed the hidden representation. Our logits are the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>U</mi><mi mathvariant="normal">‚ä§</mi></msup><msub><mtext mathvariant="bold">h</mtext><mi>i</mi></msub><mo>‚àà</mo><msup><mi mathvariant="double-struck">R</mi><mi>C</mi></msup></mrow><annotation encoding="application/x-tex">U^\top \textbf{h}_i \in \mathbb{R}^C</annotation></semantics></math></span><!----></span>. Note how we choose <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>th token vector to get our output logits. <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span> corresponds to the last token in the decoder input which is not a padding token, and we get the logits from this token because we want to predict the next word after this last word. Now we can pass through a softmax to get the output probability distribution over our vocabulary. To actually get the prediction, we sample from this distribution. In the paper they play around with beam search for some of their results, but I‚Äôm just going to keep things simple and take the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>argmax</mtext></mrow><annotation encoding="application/x-tex">\text{argmax}</annotation></semantics></math></span><!----></span> from the distribution over our vocabulary.</p> <p><img src="/images/transformers2/transformersDiagramPostDecoder.png" alt="PostDecoder" title="PostDecoder"></p> <p>The decoder is run autoregressively until it outputs the &lt;EOS> token, at which point the translation is finished!</p> <blockquote style="background-color: #ecfeec; border-color: #528852; font-style: inherit;">We now have our final representation $H$ that has information about the original sentence in English ("Bobby likes blackberries."), the current German translation ("Bobby mag"), and the probable next word. We will first apply the unembedding to the third row of $H$ as follows, <!----><math><mrow><mi>P</mi><mo form="prefix" stretchy="false">(</mo><msub><mtext>ùê≤</mtext><mn>4</mn></msub><mo form="postfix" stretchy="false">)</mo><mo>=</mo></mrow><mrow><mtext>softmax</mtext><mo form="prefix" stretchy="false">(</mo><msup><mi>U</mi><mi>‚ä§</mi></msup><msub><mtext>ùê°</mtext><mn>3</mn></msub><mo form="postfix" stretchy="false">)</mo><mo>‚àà</mo></mrow><mrow><msup><mi>‚Ñù</mi><mi>C</mi></msup></mrow></math><!----> <!--[!--><!--]--><!---->. Now we can sample from this distribution to get <!----><math><mrow><msub><mtext>ùê≤</mtext><mn>4</mn></msub><mo>=</mo></mrow><mrow><msub><mtext>ùêû</mtext><mrow><mtext>argmax</mtext><mo form="prefix" stretchy="false">(</mo><mi>P</mi><mo form="prefix" stretchy="false">(</mo><msub><mtext>ùê≤</mtext><mn>4</mn></msub><mo form="postfix" stretchy="false">)</mo><mo form="postfix" stretchy="false" lspace="0em" rspace="0em">)</mo></mrow></msub><mo>‚àà</mo></mrow><mrow><msup><mi>‚Ñù</mi><mi>C</mi></msup></mrow></math><!----> <!--[!--><!--]--><!---->. If the model was trained well, this <!----><math><msub><mtext>ùê≤</mtext><mn>4</mn></msub></math><!----> <!--[!--><!--]--><!----> standard basis vector will likely corresponds to the token "Brom", which is the correct next token in the translation.</blockquote> <h1>Training a Transformer</h1> <p>To train the described configuration of the transformer, the authors used the WMT 2014 English-German dataset. The Adam optimizer was also used with <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mn>1</mn></msub><mo>=</mo><mn>0.9</mn><mo separator="true">,</mo><mtext>¬†</mtext><msub><mi>Œ≤</mi><mn>2</mn></msub><mo>=</mo><mn>0.98</mn><mo separator="true">,</mo><mtext>¬†</mtext><mi>œµ</mi><mo>=</mo><msup><mn>10</mn><mrow><mo>‚àí</mo><mn>9</mn></mrow></msup></mrow><annotation encoding="application/x-tex">\beta_1 = 0.9, \ \beta_2 = 0.98, \ \epsilon = 10^{-9}</annotation></semantics></math></span><!----></span>. Additionally, they increased the learning rate linearly for the first 4,000 training steps, and then employed inverse square root learning rate decay for the remaining 96,000 training steps. Here is the mathematical formula used: <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi><mi>r</mi><mi>a</mi><mi>t</mi><mi>e</mi><mo>=</mo><msubsup><mi>d</mi><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi></mrow><mrow><mo>‚àí</mo><mn>0.5</mn></mrow></msubsup><mo>‚ãÖ</mo><mi>min</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi mathvariant="normal">_</mi><mi>n</mi><mi>u</mi><msup><mi>m</mi><mrow><mo>‚àí</mo><mn>0.5</mn></mrow></msup><mo separator="true">,</mo><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><mi mathvariant="normal">_</mi><mi>n</mi><mi>u</mi><mi>m</mi><mo>‚ãÖ</mo><mi>w</mi><mi>a</mi><mi>r</mi><mi>m</mi><mi>u</mi><mi>p</mi><mi mathvariant="normal">_</mi><mi>s</mi><mi>t</mi><mi>e</mi><mi>p</mi><msup><mi>s</mi><mrow><mo>‚àí</mo><mn>1.5</mn></mrow></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">lrate = d_{model}^{-0.5} \cdot \min ( step\_num^{-0.5}, step\_num \cdot warmup\_steps^{-1.5} )</annotation></semantics></math></span><!----></span></p> <p>If you are curious about the details of why learning rate decay works, please see <a href="https://arxiv.org/pdf/1908.01878.pdf" rel="nofollow">this great paper</a>.</p> <p>They also used residual dropout, applied to the output of each sub-layer and the input into the encoder and decoder (the sums of the embeddings and positional encodings).</p> <p>The paper has details on the performance.</p> <h1>Variants on the Original Architecture</h1> <p>There are several variants on the original transformer architecture, most notably encoder only transformers (like BERT) and decoder only transformers (like GPT).</p> <h2>Encoder Only Transformers</h2> <p>There is a class of transformer models that are often referred to as encoder only transformers. Most commonly, this refers to <a href="https://arxiv.org/pdf/1810.04805.pdf" rel="nofollow">BERT models</a>. BERT stands for Bidirectional Encoder Representations from Transformers. BERT essentially just takes the encoder part of the transformer, scales it up and fine tunes it on downstream tasks. The encoder is bidirectional because it lacks any sort of masked attention. This enables the model to use information from further down in the sentence to create representations for an earlier word.</p> <p>One of the main tasks this model is pretrained on is masked language modeling, where the model is fed an input sentence with a random word masked out, and the model has to figure out what the word is. Here is an example input sentence and the appropriate output.</p> <p>Input: ‚ÄúTommy, who loves soccer, just &lt;MASK> a goal.‚Äù
Desired Output: ‚ÄúTommy, who loves soccer, just scored a goal.‚Äù</p> <p>However, in 2023, the hype is much more around decoder only transformers.</p> <h2>Decoder Only Transformers</h2> <p><a href="https://arxiv.org/pdf/1801.10198.pdf" rel="nofollow">Decoder only transformers</a> take the original transformer architecture, throw away the encoder, and only keep the decoder. The multi-head attention is removed and we only keep the masked multi-head self-attention because we no longer can get the key and value matrices from the encoder. This architecture can be trained with next token prediction, where the model is fed part of a sentence, and tries to generate the next word. At test time, the model autoregressively generates the output sentence.</p> <p>A series of popular decoder only transformers is the <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="nofollow">GPT</a> series of models (<a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="nofollow">GPT</a>, <a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="nofollow">GPT-2</a>, <a href="https://arxiv.org/pdf/2005.14165.pdf" rel="nofollow">GPT-3</a>, and soon <a href="https://uxplanet.org/gpt-4-facts-rumors-and-expectations-about-next-gen-ai-model-52a4ddcd662a" rel="nofollow">GPT-4</a>). With the original GPT, the paradigm was still to pretrain the model on lots of general data, and then fine tune it for specific tasks. Nowadays, the paradigm has gone away from fine tuning and to prompting. There are many resources online which explain this and you can play around with it yourself with <a href="https://chat.openai.com/" rel="nofollow">Chat-GPT</a>.</p> <h1>Conclusion</h1> <p>I hope this post sheds a bit of light on how transformers work! There are still many questions I haven‚Äôt answered here (some of which have answers in the literature, some of which don‚Äôt), such as: Why are transformers better than other architectures? How can transformers be used with other modalities? What algorithms do transformers learn? What are the limitations of the transformer architecture? How far will scaling take us?</p> <p>I might try to answer some of these in a future blog post. But for now, stay calm and code on üòé.</p> <h1>References:</h1> <ul><li><a href="https://arxiv.org/pdf/1706.03762.pdf" rel="nofollow">Attention Is All You Need</a></li> <li><a href="https://berkan.xyz/posts/2022/01/transformersPart1/" rel="nofollow">Last Post on Attention</a></li> <li><a href="https://github.com/hyunwoongko/transformer" rel="nofollow">A Good Implementation</a></li> <li><a href="https://machinelearningmastery.com/transduction-in-machine-learning/" rel="nofollow">Transduction in ML</a></li> <li><a href="https://colah.github.io/posts/2015-08-Understanding-LSTMs/" rel="nofollow">Understanding LSTMs</a></li> <li><a href="https://arxiv.org/pdf/1508.07909.pdf" rel="nofollow">Byte-Pair Encoding Paper</a></li> <li><a href="https://towardsdatascience.com/byte-pair-encoding-subword-based-tokenization-algorithm-77828a70bee0" rel="nofollow">Byte-Pair Encoding explanation</a></li> <li><a href="https://arxiv.org/pdf/1608.05859.pdf" rel="nofollow">Using the Output Embedding to Improve Language Models</a></li> <li><a href="http://jalammar.github.io/illustrated-transformer/" rel="nofollow">Illustrated Transformer</a></li> <li><a href="https://transformer-circuits.pub/2021/framework/index.html#residual-comms" rel="nofollow">A Mathematical Framework for Transformer Circuits</a></li> <li><a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem" rel="nofollow">Vanishing Gradients Problem</a></li> <li><a href="https://eli.thegreenplace.net/2016/the-softmax-function-and-its-derivative/" rel="nofollow">Derivative of Softmax</a></li> <li><a href="https://stats.stackexchange.com/questions/421935/what-exactly-are-keys-queries-and-values-in-attention-mechanisms" rel="nofollow">What are the queries, keys, and values?</a></li> <li><a href="https://arxiv.org/pdf/1607.06450.pdf" rel="nofollow">Layer normalization</a></li> <li><a href="https://paperswithcode.com/method/layer-normalization" rel="nofollow">Papers With Code Layer Norm</a></li> <li><a href="https://arxiv.org/pdf/1502.03167.pdf" rel="nofollow">Batch Normalization</a></li> <li><a href="https://leimao.github.io/blog/Layer-Normalization/" rel="nofollow">Mathematical Explanation</a></li> <li><a href="https://arxiv.org/pdf/1606.08415.pdf" rel="nofollow">GeLUs</a></li> <li><a href="https://transformer-circuits.pub/2022/solu/index.html" rel="nofollow">SoLU</a></li> <li><a href="https://stackoverflow.com/questions/58127059/how-to-understand-masked-multi-head-attention-in-transformer" rel="nofollow">Understanding Masked Multi-Head Attention</a></li> <li><a href="https://arxiv.org/pdf/1908.01878.pdf" rel="nofollow">How Does Learning Rate Decay Help Modern Neural Networks?</a></li> <li><a href="https://arxiv.org/pdf/1810.04805.pdf" rel="nofollow">BERT models</a></li> <li><a href="https://arxiv.org/pdf/1801.10198.pdf" rel="nofollow">Decoder only transformers</a></li> <li><a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="nofollow">GPT</a></li> <li><a href="https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf" rel="nofollow">GPT-2</a></li> <li><a href="https://arxiv.org/pdf/2005.14165.pdf" rel="nofollow">GPT-3</a></li> <li><a href="https://uxplanet.org/gpt-4-facts-rumors-and-expectations-about-next-gen-ai-model-52a4ddcd662a" rel="nofollow">GPT-4 Announcement</a></li> <li><a href="https://chat.openai.com/" rel="nofollow">Chat-GPT</a></li> <li><a href="http://nlp.seas.harvard.edu/annotated-transformer/" rel="nofollow">The Annotated Transformer</a></li> <li><a href="https://johnthickstun.com/docs/transformers.pdf" rel="nofollow">A Mathematical Guide to Transfomers</a></li></ul><!----><!----></div></div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_94jwj8 = {
						base: new URL("..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.DI11qAQK.js"),
						import("../_app/immutable/entry/app.D0zaimvy.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 7],
							data: [null,null],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
