<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="../favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,200..800;1,6..72,200..800&family=Roboto+Serif:ital,opsz,wght@0,8..144,100..900;1,8..144,100..900&family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
		
		<link href="../_app/immutable/assets/0.uY5HYXPu.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.DI11qAQK.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B2-O15i9.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BuAUvOro.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B_P4XA_-.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.D0zaimvy.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D8__v_G3.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/hqIbxRWK.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DKJMLC42.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DAMHlmQC.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BIEbWpQy.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.D_zbH_5b.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DhGDfn95.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/4.DxoR_ckz.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BntcpoEZ.js">
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><div class="max-w-[100ch] mx-auto px-4 mb-10"><nav class="flex flex-col md:flex-row md:items-center md:justify-between py-4 border-b border-olive mb-3"><a href="/" class="text-olive font-bold text-2xl md:text-2xl no-underline hover:underline mb-4 md:mb-0">Berkan</a> <div class="flex flex-col md:flex-row md:space-x-6 space-y-4 md:space-y-0"><a href="/blog" class="text-olive no-underline hover:underline">Blog</a> <a href="/projects" class="text-olive no-underline hover:underline">Projects</a> <a href="/publications" class="text-olive no-underline hover:underline">Publications</a> <a href="/resources" class="text-olive no-underline hover:underline">Resources</a></div></nav> <div><!----><p><em>I wrote this article for the Columbia Undergraduate Science Journal (CUSJ) and simply copy pasted it here. Click <a href="https://journals.library.columbia.edu/index.php/cusj/blog/view/366" rel="nofollow">here</a> to see it on the CUSJ website</em></p> <p>Imagine using Face-ID to unlock your phone, but having to take thousands of images of yourself in various environments, for your phone to finally recognize your face [1]. If it weren’t for few-shot learning (FSL), this would be the case. A typical modern deep neural network for object detection, trained on the state-of-the-art ImageNet dataset, might require several hundred thousand images of oboes, trumpets, and flutes to detect them, and then another several hundred thousand to detect trombones [2]. On the other hand, even children are able to differentiate between trumpets and trombones after only seeing a few examples. FSL aims to create models that can make accurate predictions after only seeing a very small training dataset [1]. However, this doesn’t mean that the models can make accurate predictions out of thin air. The models somehow need to get useful additional information.</p> <p>Researchers have identified three main approaches to FSL to get that additional information: the data, model, and algorithm approach [3]. The data approach in some way involves increasing the size of the training data for the FSL model. This can be done by translating, flipping, or modifying image samples in some other way [4] to generate more data, transforming samples from similar data sets [5], and more. By essentially increasing the data set, this approach to FSL can leverage more traditional deep neural networks that excel at making accurate predictions from large data sets. For example, if you want to classify apples, oranges, and bananas, but you only have a picture of each, you can slightly rotate training images, flip them horizontally, and crop them. All of these images are still fundamentally bananas. A traditional classifier can use these additional training examples to then make better predictions..</p> <p>The second approach, the model approach, uses insights from learned representations. Embedding learning for FSL is an example of the model approach, as a learned embedding function can embed samples into a lower-dimensional space where a simpler model that doesn’t require as much data can be used for classification [6]. One of the most popular deep learning models today (and biggest with 175 billion parameters), GPT-3 is also in this category [7]. GPT-3 is fundamentally a language model, meaning that given a prompt, it is able to predict what comes next in a text. It utilizes a new architecture called transformers [8], to learn incredibly useful data representations that it effectively leverages during test time.</p> <p>The third approach, the algorithm approach, mainly involves learning how to learn, or meta-learning. In meta-learning, there are two learning algorithms at play: a meta-learner and the regular learner. The meta-learner usually learns to create the learner to perform optimally on a task [9]. In a way, it’s similar to how parents may learn to become better parents as they have multiple children over the years, and therefore each successive child is hopefully more successful (it works better for meta-learning algorithms than for humans). This approach may be hard to understand, but it has produced some of the most exciting results in recent years [10].</p> <p>Already, FSL models have incredible real world applications for low-data drug discovery [12], voice cloning from short audio snippets [13], and other domains where there is little labeled training data. AI pioneer Geoffrey Hinton said, “I do believe deep learning is going to be able to do everything, but I do think there’s going to have to be quite a few conceptual breakthroughs” [11]. As FSL models become better and better at learning from small data sets, similar to humans, they have the potential to lead us to one of these incredible “conceptual breakthroughs”. Who knows where few-shot learning could take us in fifty years.</p> <p><strong>References:</strong></p> <ol><li>Ozsubasi, W. (2020, November 1). Few-Shot Learning (FSL): What it is &amp; its Applications. Retrieved July 16, 2021, from <a href="https://research.aimultiple.com/few-shot-learning/" rel="nofollow">https://research.aimultiple.com/few-shot-learning/</a></li> <li>ILSVRC2017. (n.d.). Retrieved July 16, 2021, from <a href="https://image-net.org/challenges/LSVRC/2017/browse-det-synsets.php" rel="nofollow">https://image-net.org/challenges/LSVRC/2017/browse-det-synsets.php</a></li> <li>Wang, Y., Yao, Q., Kwok, J., &amp; Ni, L. M. (2020). Generalizing from a few examples: A survey on few-shot learning. ArXiv:1904.05046 [Cs]. <a href="http://arxiv.org/abs/1904.05046" rel="nofollow">http://arxiv.org/abs/1904.05046</a></li> <li>Shyam, P., Gupta, S., &amp; Dukkipati, A. (2017). Attentive recurrent comparators. ArXiv:1703.00767 [Cs]. <a href="http://arxiv.org/abs/1703.00767" rel="nofollow">http://arxiv.org/abs/1703.00767</a></li> <li>Tsai, Y.-H. H., &amp; Salakhutdinov, R. (2018). Improving one-shot learning through fusing side information. ArXiv:1710.08347 [Cs]. <a href="http://arxiv.org/abs/1710.08347" rel="nofollow">http://arxiv.org/abs/1710.08347</a></li> <li>Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., &amp; Darrell, T. (2014). Caffe: Convolutional architecture for fast feature embedding. ArXiv:1408.5093 [Cs]. <a href="http://arxiv.org/abs/1408.5093" rel="nofollow">http://arxiv.org/abs/1408.5093</a></li> <li>Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J., Winter, C., … Amodei, D. (2020). Language models are few-shot learners. ArXiv:2005.14165 [Cs]. <a href="http://arxiv.org/abs/2005.14165" rel="nofollow">http://arxiv.org/abs/2005.14165</a></li> <li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &amp; Polosukhin, I. (2017). Attention is all you need. ArXiv:1706.03762 [Cs]. <a href="http://arxiv.org/abs/1706.03762" rel="nofollow">http://arxiv.org/abs/1706.03762</a></li> <li>Hospedales, T., Antoniou, A., Micaelli, P., &amp; Storkey, A. (2020). Meta-learning in neural networks: A survey. ArXiv:2004.05439 [Cs, Stat]. <a href="http://arxiv.org/abs/2004.05439" rel="nofollow">http://arxiv.org/abs/2004.05439</a></li> <li>Finn, C., Abbeel, P., &amp; Levine, S. (2017). Model-agnostic meta-learning for fast adaptation of deep networks. ArXiv:1703.03400 [Cs]. <a href="http://arxiv.org/abs/1703.03400" rel="nofollow">http://arxiv.org/abs/1703.03400</a></li> <li>Hao, K. (2020, November 3). AI pioneer Geoff Hinton: “Deep learning is going to be able to do everything.” Retrieved July 16, 2021, from <a href="https://www.technologyreview.com/2020/11/03/1011616/ai-godfather-geoffrey-hinton-deep-learning-will-do-everything/" rel="nofollow">https://www.technologyreview.com/2020/11/03/1011616/ai-godfather-geoffrey-hinton-deep-learning-will-do-everything/</a></li> <li>Altae-Tran, H., Ramsundar, B., Pappu, A. S., &amp; Pande, V. (2017). Low data drug discovery with one-shot learning. ACS Central Science, 3(4), 283–293. <a href="https://doi.org/10.1021/acscentsci.6b00367" rel="nofollow">https://doi.org/10.1021/acscentsci.6b00367</a></li> <li>Arik, S. O., Chen, J., Peng, K., Ping, W., &amp; Zhou, Y. (2018). Neural voice cloning with a few samples. ArXiv:1802.06006 [Cs, Eess]. <a href="http://arxiv.org/abs/1802.06006" rel="nofollow">http://arxiv.org/abs/1802.06006</a></li></ol><!----><!----></div></div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_94jwj8 = {
						base: new URL("..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.DI11qAQK.js"),
						import("../_app/immutable/entry/app.D0zaimvy.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 4],
							data: [null,null],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
