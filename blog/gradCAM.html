<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="../favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,200..800;1,6..72,200..800&family=Roboto+Serif:ital,opsz,wght@0,8..144,100..900;1,8..144,100..900&family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
		
		<link href="../_app/immutable/assets/0.BC1D2L3I.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.DjNdHTN9.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/CJALEPrJ.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BuAUvOro.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B_P4XA_-.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.BJien36n.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D8__v_G3.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/hqIbxRWK.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DKJMLC42.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DAMHlmQC.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BIEbWpQy.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.IkV6mckE.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DhGDfn95.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/5.cIL9C3NX.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BntcpoEZ.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/wXsJu9Ig.js">
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><div class="max-w-[100ch] mx-auto px-4 mb-10"><nav class="flex flex-col md:flex-row md:items-center md:justify-between py-4 border-b border-olive mb-3"><a href="/" class="text-olive font-bold text-2xl md:text-2xl no-underline hover:underline mb-4 md:mb-0">Berkan</a> <div class="flex flex-col md:flex-row md:space-x-6 space-y-4 md:space-y-0"><a href="/blog" class="text-olive no-underline hover:underline">Blog</a> <a href="/projects" class="text-olive no-underline hover:underline">Projects</a> <a href="/publications" class="text-olive no-underline hover:underline">Publications</a> <a href="/digitalGarden" class="text-olive no-underline hover:underline">Digital Garden</a></div></nav> <div><!----><p><em><strong>WARNING:</strong> This post assumes you have a basic understanding of neural networks and specifically Convolutional Neural Networks (CNNs). Some Python knowledge (PyTorch and NumPy mainly) would also be useful to understand the code samples.</em></p> <p><em>This post is the written extension of a presentation given for a deep learning theory reading group at Columbia.</em></p> <p>This post is an explanation of some of the basics of <a href="https://arxiv.org/pdf/1610.02391.pdf" rel="nofollow">Grad-CAM</a>. There are, however, some things described in the paper that are not mentioned here. I will start by giving an overview of CNN interpretability and describing the basics of Grad-CAM. Then I will explain how CAM works. Finally, I will get back to Grad-CAM and give a more detailed explanation.</p> <p>My implementation in PyTorch on the MNIST dataset is in a jupyter notebook and can be found in html form <a href="https://berkan.xyz/files/gradCAM.html" rel="nofollow">here</a>. The GitHub repo can be found <a href="https://github.com/berkott/cnn-interpretation-pytorch" rel="nofollow">here</a>. I will show some of the result images and code here.</p> <h1>Very Basics of Interpretability</h1> <p>Simply put, interpretability matters. And it matters for a variety of reasons. You could look at it from the perspective of AI Safety, if we have more intepretable models, perhaps we can find <a href="https://www.alignmentforum.org/posts/CzZ6Fch4JSpwCpu6C/interpretability" rel="nofollow">unknown safety problems</a> in our models that can prevent doom. This is something I may explore more in a future blog post :)</p> <p>For now though, I’m going to explain the more near term reasoning the authors of the Grad-CAM paper said for why interpretability matters. Namely they said that in order to build trust in AI systems for more meaningful integration into our lives, we need more transparent models. They mentioned how this can be useful at 3 stages of AI development in a problem:</p> <ol><li>When AI is much weaker than humans at a task, intepretability helps to identify failure modes that can help to find directions to improve the model.</li> <li>When AI is on par with human performance on a task, interpretability can help build trust.</li> <li>When AI is better than humans at a task, interpretability can help enable machine teaching where humans can learn from AI systems.</li></ol> <p>One thing Grad-CAM tries to address, is the traditional accuracy and simplicity/interpretability trade-off. Basically, if you think of models that were more popular in the past such as decision trees, they were very interpretable, but not very accurate at many tasks such as image classification or sentence translation. However, if you look at more modern deep neural networks, they can have incredible accuracy, but are often complex black boxes that are very hard to interpret. Grad-CAM aims to be an interpretability tool that can beat this trade-off.</p> <p>Another thing that Grad-CAM tries to do is to be a good visual explanation. The authors claim that a good visual explanation is high resolution and class discriminative. We will explore how Grad-CAM is both of these.</p> <p>Now we are ready to dig in. However, before talking about Grad-CAM we will discuss <a href="https://arxiv.org/pdf/1512.04150.pdf" rel="nofollow">CAM</a>.</p> <h1>Class Activation Mappings (CAM)</h1> <p>CAM aims to create class-discriminative localization maps that essentially show what the CNN looks at when making a certain classification. These heatmaps look like this:</p> <p><img src="/images/gradCAM/cam.png" alt="CAM Basics" title="CAM Basics"></p> <p>In the class activation mappings, the authors cited two key recent discoveries at the time (this paper was written in late 2015) that helped their research.</p> <p>The first is that <a href="https://arxiv.org/abs/1412.6856" rel="nofollow">Zhou et al</a> showed that various layers of convolutional neural networks actually behave as object detectors. The key word in that sentence is detectors, because the detection problem involves classification and localization. So essentially there is localization ability built into these convolutional layers, but it turns out that this ability is lost when fully-connected layers are used after the convolutional layers.</p> <p>The second discovery is that some <a href="https://arxiv.org/abs/1312.4400" rel="nofollow">popular fully-convolutional neural networks</a> came out at the time that totally avoided using full-connected layers to minimize the number of parameters, and instead use global average pooling. This global average pooling acts as a regularizer and has also been shown to retain the localization ability described before until the output layer.</p> <p>However, before describing the rest of CAM, I first want to explain <a href="https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/" rel="nofollow">Global Average Pooling (GAP)</a>.</p> <h2>Global Average Pooling</h2> <p>When GAP is used, it is typically applied to the output of the final convolutional layer. It collapses the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi><mo>×</mo><mi>w</mi><mo>×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">h \times w \times d</annotation></semantics></math></span><!----></span> output down to a <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>×</mo><mn>1</mn><mo>×</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">1 \times 1 \times d</annotation></semantics></math></span><!----></span> vector. One slightly weird thing is that in the CAM paper they don’t actually take the average of each of the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><!----></span> feature maps that come from the final convolutional layer, they just take the sum of all. <em><strong>Notation alert:</strong> I’m going to use notation that is more consistent with the Grad-CAM paper.</em></p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>F</mi><mi>k</mi></msup><mo>=</mo><msub><mo>∑</mo><mi>i</mi></msub><msub><mo>∑</mo><mi>j</mi></msub><msubsup><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>k</mi></msubsup></mrow><annotation encoding="application/x-tex">F^k = \sum_{i} \sum_{j} A_{ij}^k</annotation></semantics></math></span><!----></span></p> <ul><li>Where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>∈</mo><mo stretchy="false">{</mo><mn>1</mn><mo separator="true">,</mo><mn>2</mn><mo>…</mo><mi>d</mi><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">k \in \{ 1, 2 \ldots d \}</annotation></semantics></math></span><!----></span> representing the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><!----></span>th feature map from the final conv layer</li> <li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>F</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">F^k</annotation></semantics></math></span><!----></span> is just a number and represents GAP applied to the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><!----></span>th filter</li></ul> <p>This picture shows what’s going on:</p> <p><img src="/images/gradCAM/gap.png" alt="GAP" title="GAP"></p> <p>I implemented this as follows in PyTorch into my model for classifying MNIST digits.</p> <pre class="language-python"><!----><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">CAM_CNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>CAM_CNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>         
      nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>
        in_channels<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>              
        out_channels<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>            
        kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>              
        stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>                   
        padding<span class="token operator">=</span><span class="token number">2</span>               
      <span class="token punctuation">)</span><span class="token punctuation">,</span>                              
      nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>                      
      nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span>   
    <span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>         
      nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span>
    <span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>gap <span class="token operator">=</span> nn<span class="token punctuation">.</span>AvgPool2d<span class="token punctuation">(</span><span class="token number">7</span><span class="token punctuation">)</span> <span class="token comment"># GAP here!</span>

    self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>


  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    y <span class="token operator">=</span> self<span class="token punctuation">.</span>gap<span class="token punctuation">(</span>x<span class="token punctuation">)</span>

    y <span class="token operator">=</span> y<span class="token punctuation">.</span>view<span class="token punctuation">(</span>y<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>

    y <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
    <span class="token keyword">return</span> y<span class="token punctuation">,</span> x</code><!----></pre> <h2>Back to CAM</h2> <p>Now we can come back to CAM. CAM requires a sort of specific architecture to work. It requires the network consists of convolutional layers (these can have any sort of pooling, dropout, residual blocks etc.), then a GAP layer, and finally an output layer.</p> <p>CAM relies on the fact that we can identify the importance of certain regions of the image by projecting the weights of the output layer back onto the final convolutional feature maps. Essentially to get the CAM, we take a weighted average of the final convolutional feature maps weighted by the weights of the output class that connect to the global average pooling.</p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>L</mi><mrow><mi>C</mi><mi>A</mi><mi>M</mi></mrow><mi>c</mi></msubsup><mo>=</mo><msub><mo>∑</mo><mi>k</mi></msub><msubsup><mi>w</mi><mi>k</mi><mi>c</mi></msubsup><msup><mi>A</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">L_{CAM}^c=\sum_k w_k^c A^k</annotation></semantics></math></span><!----></span></p> <ul><li>Where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>L</mi><mrow><mi>C</mi><mi>A</mi><mi>M</mi></mrow><mi>c</mi></msubsup></mrow><annotation encoding="application/x-tex">L_{CAM}^c</annotation></semantics></math></span><!----></span> represents the CAM of output class c.</li> <li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>w</mi><mi>k</mi><mi>c</mi></msubsup></mrow><annotation encoding="application/x-tex">w_k^c</annotation></semantics></math></span><!----></span> represents the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><!----></span>th weight of output class <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><!----></span>.</li></ul> <p>Maybe this picture from the paper can help make things more clear:</p> <p><img src="/images/gradCAM/camCalc.png" alt="CAM Calc" title="CAM Calc"></p> <p>Because typically as you go though more and more convolutional layers the width and height of your feature maps decrease and your depth increases, you will likely have to upscale the CAM to the size of your image so you can nicely overlay it.</p> <p>In PyTorch I implemented this as follows, where <code>sum_res</code> is the final class specific CAM representation and has dimensions of <code>[10, 7, 7]</code>.</p> <pre class="language-python"><!----><code class="language-python"><span class="token keyword">with</span> torch<span class="token punctuation">.</span>no_grad<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  x<span class="token punctuation">,</span> y <span class="token operator">=</span> data<span class="token punctuation">[</span>data_index<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span>data_index<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>
  
  final_conv_output <span class="token operator">=</span> <span class="token number">0</span>
  cam_weights <span class="token operator">=</span> <span class="token number">0</span>

  pred<span class="token punctuation">,</span> final_conv_output <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

  <span class="token keyword">for</span> param <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">if</span> param<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">==</span> torch<span class="token punctuation">.</span>Size<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
      cam_weights <span class="token operator">=</span> param
      <span class="token keyword">break</span>

  mult_res <span class="token operator">=</span> torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>cam_weights<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">,</span> final_conv_output<span class="token punctuation">)</span>

  sum_res <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>mult_res<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span></code><!----></pre> <h2>GAP vs GMP</h2> <p>A question that might come up is, why GAP and why not Global Max Pooling (GMP). In the paper, they cite emiprical evidence that says that GAP and GMP have similar classification performance, but GAP has better localization performance. Also intuitive reasoning says that GAP will encourage the network to identify the extent of an object while GMP encourages it to find one disciminative feature.</p> <h2>Uses</h2> <p>CAM is already pretty useful! It can lend insights into failure modes (such as this <a href="https://www.jefftk.com/p/detecting-tanks" rel="nofollow">famous US Army story</a> that may or may not be true). Another thing is that it can be used as a weakly supervised model (as opposed to a strongly supervised model given bounding boxes during training time) for object localiation. The idea is that you basically draw bounding boxes around the CAM visualizations per class to get your localization. It actually performed decently well (see paper for more details).</p> <p>From my personal implementation, I understood how CAM can do both of these things. One detection that my model got wrong is this 6:</p> <p><img src="/images/gradCAM/camDigit22.png" alt="CAM Implementation Pic" title="CAM Implementation Pic"></p> <p>At first glance to me, it seems kind of obvious that it’s a 6. However, when I looked at the CAM visualization it became more clear how the model could mistake it for a 4.</p> <p><img src="/images/gradCAM/camHeatmaps22.png" alt="CAM Implementation Pic" title="CAM Implementation Pic"></p> <p><img src="/images/gradCAM/camOverlay22.png" alt="CAM Implementation Pic" title="CAM Implementation Pic"></p> <p>The top images shows the CAM heatmaps for all classes and the bottom image shows the predicted class overlaid onto the input image. For classes that don’t have a high output probability, the visualizations are less interesting, but for ones that are higher such as 6 and 4 in this example, it is more interesting. The redder regions represent regions that positively correlate with that output class. So you can see how for detecting a 4, the model doesn’t look that much at the bottom part of the 4. This can be confirmed by looking at correct detections of 4’s where the same trend can be spotted.</p> <p><img src="/images/gradCAM/camOverlay4.png" alt="CAM Implementation Pic" title="CAM Implementation Pic"></p> <p>Now looking back at the original 6, it is more understandable how the model could mistake it for a 4, if it’s definition of a 4 doesn’t value the bottom part of the 4 much.</p> <p>CAM also seemed to work pretty well as a weekly supervised object localization model. Of course, in the MNIST dataset, location doesn’t matter too much as the digits are mostly centered, but it’s still clear by this example of a 7 that the model seems to do a good job of this:</p> <p><img src="/images/gradCAM/camOverlay0.png" alt="CAM Implementation Pic" title="CAM Implementation Pic"></p> <p>If you drew a good bounding box around the redder regions of the image, you would localize the 7 pretty well. Another cool thing you can see here is that the model doesn’t seem to value what happens around the middle of the slanted line in the 7, this could be because some of the 7’s in the dataset contain dashes there and some don’t, so the model might have learned not to really pay attention to that.</p> <h2>Areas of Improvement</h2> <p>However, CAM is by no means perfect. For one, the output visualizations are not very high resolution due to the upscaling step. Additionally, CAM only works for a relatively small set of possible models, which means it is still stuck in the accuracy vs interpretability trade-off as there could be other models with better accuracy that CAM can’t be applied on, meaning they are less interpretable. Further issues that both CAM and Grad-CAM have will be discussed below.</p> <h1>Gradient Based Class Activation Mappings (Grad-CAM)</h1> <p>Now we can finally talk about Grad-CAM! The basic idea is the same as CAM, we are again trying to obtain class-discriminative localization maps. Only now, we are using gradient information (hence Gradient Based CAM) flowling into the last convolutional layer to get importance values for each neuron for a particular decision. This is instead of using the weights to the output layer from GAP as seen in CAM. Due to the nature of gradients, Grad-CAM can be used for any convolutional layer. However, it is typically uses the last convolutional layer because <a href="https://arxiv.org/pdf/1512.02017.pdf" rel="nofollow">deeper representations</a> in CNNs have been found to capture <a href="https://arxiv.org/pdf/1206.5538.pdf" rel="nofollow">higher level visual concepts</a> which is what we are after.</p> <p>Simply put, this is what Grad-CAM does:</p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>L</mi><mrow><mi>G</mi><mi>r</mi><mi>a</mi><mi>d</mi><mo>−</mo><mi>C</mi><mi>A</mi><mi>M</mi></mrow><mi>c</mi></msubsup><mo>=</mo><mi>R</mi><mi>e</mi><mi>L</mi><mi>U</mi><mo stretchy="false">(</mo><msub><mo>∑</mo><mi>k</mi></msub><msubsup><mi>α</mi><mi>k</mi><mi>c</mi></msubsup><msup><mi>A</mi><mi>k</mi></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">L_{Grad-CAM}^c=ReLU(\sum_k \alpha_k^c A^k)</annotation></semantics></math></span><!----></span></p> <ul><li>We apply the ReLU because we only care about features that have a positive influence on the class of interest, negative features likely belong to other classes.</li> <li>Basically substituting <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi></mrow><annotation encoding="application/x-tex">w</annotation></semantics></math></span><!----></span> for the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><!----></span> term.</li> <li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>A</mi><mi>k</mi></msup></mrow><annotation encoding="application/x-tex">A^k</annotation></semantics></math></span><!----></span> is the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><!----></span>th feature map of the final convolutional layer outputs.</li></ul> <p>The <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>α</mi><mi>k</mi><mi>c</mi></msubsup></mrow><annotation encoding="application/x-tex">\alpha_k^c</annotation></semantics></math></span><!----></span> term comes from here:</p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>α</mi><mi>k</mi><mi>c</mi></msubsup><mo>=</mo><mfrac><mn>1</mn><mi>Z</mi></mfrac><msub><mo>∑</mo><mi>i</mi></msub><msub><mo>∑</mo><mi>j</mi></msub><mfrac><mrow><mi mathvariant="normal">∂</mi><msup><mi>y</mi><mi>c</mi></msup></mrow><mrow><mi mathvariant="normal">∂</mi><msubsup><mi>A</mi><mrow><mi>i</mi><mi>j</mi></mrow><mi>k</mi></msubsup></mrow></mfrac></mrow><annotation encoding="application/x-tex">\alpha_k^c = \frac{1}{Z}\sum_i \sum_j \frac{\partial y^c}{\partial A_{ij}^k }</annotation></semantics></math></span><!----></span></p> <ul><li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mi>c</mi></msup></mrow><annotation encoding="application/x-tex">y^c</annotation></semantics></math></span><!----></span> represents the output of the final layer <strong>before the softmax</strong> for a specific class.</li> <li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Z</mi></mrow><annotation encoding="application/x-tex">Z</annotation></semantics></math></span><!----></span> is a proportionality constant used for normalization (it is the number of pixels in the feature map).</li> <li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>α</mi><mi>k</mi><mi>c</mi></msubsup></mrow><annotation encoding="application/x-tex">\alpha_k^c</annotation></semantics></math></span><!----></span> represents a partial linearization of the deep network downstream from <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>A</mi></mrow><annotation encoding="application/x-tex">A</annotation></semantics></math></span><!----></span>.</li></ul> <p>You can see how this <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi></mrow><annotation encoding="application/x-tex">\alpha</annotation></semantics></math></span><!----></span> term is very similar to GAP. We are taking the average of the gradients of the output class with respect to each pixel in the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><!----></span>th feature map, essentially giving us the “importance” of that feature map to the final prediction.</p> <p>The paper further shows how Grad-CAM is actually a strict generalization of CAM.</p> <p>In my personal implementation, I used the following model (note how it doesn’t have the GAP like my CAM implementation model had).</p> <pre class="language-python"><!----><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Grad_CAM_CNN</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span>Grad_CAM_CNN<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>         
      nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>
        in_channels<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
        out_channels<span class="token operator">=</span><span class="token number">16</span><span class="token punctuation">,</span>
        kernel_size<span class="token operator">=</span><span class="token number">5</span><span class="token punctuation">,</span>
        stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span>
        padding<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span>
      <span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span>kernel_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>
      nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">16</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
      nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">,</span>
    <span class="token punctuation">)</span>

    <span class="token comment"># fully connected layer, output 10 classes</span>
    self<span class="token punctuation">.</span>out <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">32</span> <span class="token operator">*</span> <span class="token number">7</span> <span class="token operator">*</span> <span class="token number">7</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>

    self<span class="token punctuation">.</span>input_gradients <span class="token operator">=</span> <span class="token boolean">None</span>

  <span class="token keyword">def</span> <span class="token function">input_gradients_hook</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> grad<span class="token punctuation">)</span><span class="token punctuation">:</span>
    self<span class="token punctuation">.</span>input_gradients <span class="token operator">=</span> grad
  
  <span class="token keyword">def</span> <span class="token function">get_input_gradients</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>input_gradients

  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token punctuation">)</span>
    x<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

    x<span class="token punctuation">.</span>register_hook<span class="token punctuation">(</span>self<span class="token punctuation">.</span>input_gradients_hook<span class="token punctuation">)</span>

    x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    
    <span class="token comment"># flatten the output of conv2 to (batch_size, 32 * 7 * 7)</span>
    y <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    y <span class="token operator">=</span> self<span class="token punctuation">.</span>out<span class="token punctuation">(</span>y<span class="token punctuation">)</span>
    
    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>y<span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">,</span> x</code><!----></pre> <p>To get the actual Grad-CAM visualizations, I did the following:</p> <pre class="language-python"><!----><code class="language-python">model<span class="token punctuation">.</span><span class="token builtin">eval</span><span class="token punctuation">(</span><span class="token punctuation">)</span>

x<span class="token punctuation">,</span> y <span class="token operator">=</span> data<span class="token punctuation">[</span>data_index<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> data<span class="token punctuation">[</span>data_index<span class="token punctuation">]</span><span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span>

class_specific_alphas <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float64<span class="token punctuation">)</span>
class_specific_alphas <span class="token operator">=</span> class_specific_alphas<span class="token punctuation">.</span>new_zeros<span class="token punctuation">(</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> c <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  model<span class="token punctuation">.</span>zero_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>
  pred<span class="token punctuation">,</span> final_conv_output <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

  final_conv_output<span class="token punctuation">.</span>requires_grad_<span class="token punctuation">(</span><span class="token punctuation">)</span>
  final_conv_output<span class="token punctuation">.</span>retain_grad<span class="token punctuation">(</span><span class="token punctuation">)</span>

  one_hot_output <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> pred<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>
  one_hot_output<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">[</span>c<span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">1</span>

  pred<span class="token punctuation">.</span>backward<span class="token punctuation">(</span>gradient<span class="token operator">=</span>one_hot_output<span class="token punctuation">,</span> retain_graph<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

  class_specific_alphas<span class="token punctuation">[</span>c<span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>final_conv_output<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>

class_specific_alphas <span class="token operator">=</span> torch<span class="token punctuation">.</span>div<span class="token punctuation">(</span>class_specific_alphas<span class="token punctuation">,</span> <span class="token number">7</span> <span class="token operator">*</span> <span class="token number">7</span><span class="token punctuation">)</span>

pred<span class="token punctuation">,</span> final_conv_output <span class="token operator">=</span> model<span class="token punctuation">(</span>x<span class="token punctuation">[</span><span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

mult_res <span class="token operator">=</span> torch<span class="token punctuation">.</span>mul<span class="token punctuation">(</span>class_specific_alphas<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span><span class="token punctuation">,</span> final_conv_output<span class="token punctuation">)</span>
sum_res <span class="token operator">=</span> nn<span class="token punctuation">.</span>functional<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span>mult_res<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span></code><!----></pre> <p>Where <code>sum_res</code> is once again the final class specific CAM representation and has dimensions of <code>[10, 7, 7]</code>.</p> <p>Now here you might be thinking, <em>why is this guy using <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mi>c</mi></msup></mrow><annotation encoding="application/x-tex">y^c</annotation></semantics></math></span><!----></span> <strong>after the softmax</strong>, I thought it was supposed to be <strong>before the softmax</strong></em>? And you would be right, the paper does say before. However, I found that using <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>y</mi><mi>c</mi></msup></mrow><annotation encoding="application/x-tex">y^c</annotation></semantics></math></span><!----></span> <strong>after the softmax</strong> worked better. I believe this is because when calculating the gradient backwards I am setting the output to 0 for all classes and 1 for the target class. However, without the softmax, the outputs of the model are very far away from 0 and 1 for the other and target classes. I think (I am not sure) that this results in the final Grad-CAM output having most values that are 0 or less as the typical output for the target class is much higher than 1 and for the other classes is much less than 0 so the gradients are negative. Then when applying the ReLU to the Grad-CAM some images were just totally 0. But with the softmax applied the final outputs look much better. Perhaps some sort of normalization within the network could help.</p> <h2>Guided Grad-CAM</h2> <p>So far, Grad-CAM seems great! Because of the gradients we can now apply this visualization technique to a much wider range of models, sort of allowing us to beat the accuracy vs interpretability trade-off. But another thing we wanted to achive is to obtain high resolution visual explanations. To do this, we need Guided Grad-CAM.</p> <p>Guided Grad-CAM relies on <a href="https://arxiv.org/pdf/1412.6806.pdf" rel="nofollow">Guided Backpropagation</a>. Guided backprop is actually a very simple concept. We are trying to find how each pixel in the input for a specific class contributes to the output. Therefore we can propagate the gradient backwards all the way back to the input image. However, we want to set all negative gradients to 0 because we only care about pixels that positively influence our outputs. This gets us an image which for each pixel essentially represents how much that pixel influences the output. It looks like this:</p> <p><img src="/images/gradCAM/guidedBackprop.png" alt="Guided Backprop" title="Guided Backprop"></p> <p>The thing about guided backprop is that it is not class discriminative. We can, however, take the outputs from guided backprop (which is high resolution) and elementwise multiply it by the upscaled class discriminative Grad-CAM visualizations to get Guided Grad-CAM which is both high resolution and class discriminative! This is what that looks like:</p> <p><img src="/images/gradCAM/guidedGradCAM.png" alt="Guided Grad-CAM" title="Guided Grad-CAM"></p> <p>The Grad-CAM paper describes many other things including Grad-CAM for visual question answering, weakly-supervised localization and segmentation, and more. I won’t go into these but I will show one cool picture that gives a good overview of Grad-CAM.</p> <p><img src="/images/gradCAM/gradCAMOverview.png" alt="Grad-CAM Overview" title="Grad-CAM Overview"></p> <p>In my implementation, I’m <strong>not</strong> actually implementing guided backprop. I am just computing the gradients of the inputs w.r.t. the output predictions, I am not applying a ReLU to the gradients as they are propagating backwards which is what guided backprop does. I found that for this simple MNIST example it is sufficient not to use strict guided backprop. It looks as follows:</p> <p><img src="/images/gradCAM/gradCAMDigit1.png" alt="Grad-CAM Implementation Pic" title="Grad-CAM Implementation Pic"></p> <p><img src="/images/gradCAM/gradCAMGradient1.png" alt="Grad-CAM Implementation Pic" title="Grad-CAM Implementation Pic"></p> <h2>Uses</h2> <p>Grad-CAM can be used for many things, I’ll just mention a few of them:</p> <ul><li>Grad-CAM can lend insights into failure modes. This picture hows how the authors did this for the VGG-16 model.</li></ul> <p><img src="/images/gradCAM/gradCAMFailureModes.png" alt="Grad-CAM Failure Modes" title="Grad-CAM Failure Modes"></p> <ul><li>It can do impressive weakly-supervised object localization</li> <li>It can help identify biases in models and allow the models to achieve better generalization.</li></ul> <p>In my personal implementation, like for CAM, I found Grad-CAM useful for lending insights into failure modes and for weakly-supervised object localization. It’s impossible harder to identify the types of racial or similar biases in the MNIST dataset so I didn’t try to do that.</p> <p>The best example for understanding a failure mode I could find was this digit which is apparently supposed to be an 8. The model detected it as a 2, however.</p> <p><img src="/images/gradCAM/gradCAMDigit582.png" alt="Grad-CAM Implementation Pic" title="Grad-CAM Implementation Pic"></p> <p><img src="/images/gradCAM/gradCAMHeatmaps582.png" alt="Grad-CAM Implementation Pic" title="Grad-CAM Implementation Pic"></p> <p><img src="/images/gradCAM/gradCAMOverlay582.png" alt="Grad-CAM Implementation Pic" title="Grad-CAM Implementation Pic"></p> <p><img src="/images/gradCAM/gradCAMGradient582.png" alt="Grad-CAM Implementation Pic" title="Grad-CAM Implementation Pic"></p> <p><img src="/images/gradCAM/gradCAMGuided582.png" alt="Grad-CAM Implementation Pic" title="Grad-CAM Implementation Pic"></p> <p>Looking at the “Guided” (I’m using quotes because it is technically not guided backprop just the gradient of the input, explained more above) Grad-CAM output, it is clear how that can be mistaken as a 2. The heatmap also shows that the model isn’t paying attention to the bottom of the digit much to classify it as a 2.</p> <p>For the weakly supervised object localization it is clear that you could draw a relatively suitable bounding box to localize the character. This worked better for some input images than other in my implementation. This 4 is an example of a digit for which it worked pretty well.</p> <p><img src="/images/gradCAM/gradCAMDigit19.png" alt="Grad-CAM Implementation Pic" title="Grad-CAM Implementation Pic"></p> <p><img src="/images/gradCAM/gradCAMHeatmaps19.png" alt="Grad-CAM Implementation Pic" title="Grad-CAM Implementation Pic"></p> <p><img src="/images/gradCAM/gradCAMOverlay19.png" alt="Grad-CAM Implementation Pic" title="Grad-CAM Implementation Pic"></p> <p><img src="/images/gradCAM/gradCAMGradient19.png" alt="Grad-CAM Implementation Pic" title="Grad-CAM Implementation Pic"></p> <p><img src="/images/gradCAM/gradCAMGuided19.png" alt="Grad-CAM Implementation Pic" title="Grad-CAM Implementation Pic"></p> <p>One cool thing to note is how the Grad-CAM model had better accuracy on the test data than the CAM model. This is likely due to the greater freedom for model choice that Grad-CAM allows. Of course, this is not very rigorous and I could have made my CAM model much better than my simple Grad-CAM model probably by maybe adding more conv layers or trying some regularization techniques or something, but the point is that even from this simple toy example, we can see that Grad-CAM does a better job of beating the accuracy vs. interpretability trade-off.</p> <h2>Areas of Improvement</h2> <p>Even though Grad-CAM fixes some issues that CAM had, it is still not perfect. Heatmaps are a step in the right direction, but still may fail to explain complex relationships in images. There are <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ge_A_Peek_Into_the_Reasoning_of_Neural_Networks_Interpreting_With_CVPR_2021_paper.pdf" rel="nofollow">more modern techniques</a> that can do a better job of this. Also Grad-CAM is only for computer vision, there are many other areas of AI that also need interpretation.</p> <h1>Conclusion</h1> <p>To me, the simplicity of Grad-CAM is really exciting. I find interpretability as a whole a really fascinating and seemingly important field and I can’t wait to read and write more about it in the future 🤓.</p> <h1>References:</h1> <ul><li><a href="https://arxiv.org/pdf/1512.04150.pdf" rel="nofollow">CAM</a></li> <li><a href="https://arxiv.org/pdf/1610.02391.pdf" rel="nofollow">Grad-CAM</a></li> <li><a href="https://arxiv.org/pdf/1412.6806.pdf" rel="nofollow">Guided Backpropagation</a></li> <li>Some other papers and articles were cited in the text above.</li></ul> <p>My Implementation <a href="https://berkan.xyz/files/gradCAM.html" rel="nofollow">HTML</a> and <a href="https://github.com/berkott/cnn-interpretation-pytorch" rel="nofollow">GitHub Repo</a>.</p><!----><!----></div></div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_11bpjve = {
						base: new URL("..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.DjNdHTN9.js"),
						import("../_app/immutable/entry/app.BJien36n.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 5],
							data: [null,null],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
