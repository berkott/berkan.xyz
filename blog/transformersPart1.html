<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8" />
		<link rel="icon" href="../favicon.png" />
		<meta name="viewport" content="width=device-width, initial-scale=1" />
		<link rel="preconnect" href="https://fonts.googleapis.com">
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
		<link href="https://fonts.googleapis.com/css2?family=Newsreader:ital,opsz,wght@0,6..72,200..800;1,6..72,200..800&family=Roboto+Serif:ital,opsz,wght@0,8..144,100..900;1,8..144,100..900&family=Roboto:ital,wght@0,100..900;1,100..900&display=swap" rel="stylesheet">
		
		<link href="../_app/immutable/assets/0.uY5HYXPu.css" rel="stylesheet">
		<link rel="modulepreload" href="../_app/immutable/entry/start.DI11qAQK.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B2-O15i9.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BuAUvOro.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/B_P4XA_-.js">
		<link rel="modulepreload" href="../_app/immutable/entry/app.D0zaimvy.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/D8__v_G3.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/hqIbxRWK.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DKJMLC42.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DAMHlmQC.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BIEbWpQy.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/0.D_zbH_5b.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/DhGDfn95.js">
		<link rel="modulepreload" href="../_app/immutable/nodes/6.B6Cfef_H.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/BntcpoEZ.js">
		<link rel="modulepreload" href="../_app/immutable/chunks/wXsJu9Ig.js">
	</head>
	<body data-sveltekit-preload-data="hover">
		<div style="display: contents"><!--[--><!--[--><!----><div class="max-w-[100ch] mx-auto px-4 mb-10"><nav class="flex flex-col md:flex-row md:items-center md:justify-between py-4 border-b border-olive mb-3"><a href="/" class="text-olive font-bold text-2xl md:text-2xl no-underline hover:underline mb-4 md:mb-0">Berkan</a> <div class="flex flex-col md:flex-row md:space-x-6 space-y-4 md:space-y-0"><a href="/blog" class="text-olive no-underline hover:underline">Blog</a> <a href="/projects" class="text-olive no-underline hover:underline">Projects</a> <a href="/publications" class="text-olive no-underline hover:underline">Publications</a> <a href="/resources" class="text-olive no-underline hover:underline">Resources</a></div></nav> <div><!----><p><em><strong>WARNING:</strong> This post assumes you have a basic understanding of neural networks and specifically Recurrent Neural Networks (RNNs).</em></p> <p>There are some things in my life, that I‚Äôve heard about many times, but never really understood. Transformers are one of those things. That‚Äôs why, in this series of posts, I‚Äôm going to give all the building blocks that took me from a decent understanding of neural networks and RNNs but no understanding of Transformers, to a basic technical understanding of Transformers. I‚Äôm going to start with the building blocks of Transformers, namely attention mechanisms. I‚Äôm going to base most of this post on the paper <a href="https://arxiv.org/pdf/1409.0473.pdf" rel="nofollow">Neural Machine Translation by Jointly Learning to Align and Translate</a>, which first introduced attention mechanisms in 2014 (the year Germany won the world cup üòÉ‚öΩÔ∏è).</p> <p>I have not yet implemented this paper, but I found this <a href="https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb" rel="nofollow">good PyTorch implementation</a> online. I might come back to this post and add my own implementation üôÉ.</p> <p>I want to start by giving relevant background to the problem this paper aims to address.</p> <h1>Neural Machine Translation</h1> <p>Neural machine translation aims to take an input sentence comprised of the input words <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>x</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi>x</mi><msub><mi>T</mi><mi>x</mi></msub></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{x} = (x_1, \ldots, x_{T_x})</annotation></semantics></math></span><!----></span> and translate it into an output sentence <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">y</mi><mo>=</mo><mo stretchy="false">(</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi>y</mi><msub><mi>T</mi><mi>y</mi></msub></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\mathbf{y} = (y_1, \ldots, y_{T_y})</annotation></semantics></math></span><!----></span> in a different language, where  using only a single (<em>They say single in the paper but technically their model comprises of multiple neural networks as you will soon see. I guess sometimes these things are just a little vague, but I think you get the idea</em>), pretty large, neural network. I‚Äôm now going to introduce a recurring translation example used in the paper.</p> <p><strong>English Input Sentence:</strong></p> <blockquote><p>An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure, based on his status as a health care worker at a hospital.</p></blockquote> <p><strong>A Reference Translation:</strong></p> <blockquote><p>Le privilege d‚Äôadmission est le droit d‚Äôun medecin, en vertu de son statut de membre soignant d‚Äôun hopital, d‚Äôadmettre un patient dans un hopital ou un centre medical afin d‚Äôy delivrer un ¬¥diagnostic ou un traitement.</p></blockquote> <p><em>Warning:</em> The accents on top of some of the letters have disappeared during the copy/paste.</p> <p>Now I don‚Äôt know French. So I‚Äôm just going to trust some of the authors analysis on the accuracy of their model‚Äôs translations. However, if I were trying to implement this myself on a lot of data, it would be really useful to automatically gauge how good a generated translation is. Luckily, there is something for this, called the <a href="https://aclanthology.org/P02-1040.pdf" rel="nofollow">BLEU Score</a>!</p> <h2>BLEU Score</h2> <p>The <a href="https://aclanthology.org/P02-1040.pdf" rel="nofollow">paper</a> does a really good job of explaining how they derived the BLEU score, I‚Äôm just going to give a brief overview.</p> <p>The core assumption that the BLEU (BiLingual Evaluation Understudy) score paper makes, is that the closer a machine translation is to a professional human translation, the better it is. So really all the BLEU score does is measure the <strong>closeness</strong> of a machine translation to one or more reference human translations.</p> <p>Essentially, BLEU tries to compare <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">n \text{-} gram</annotation></semantics></math></span><!----></span> (basically just sequences of <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><!----></span> characters) in the machine translation to the reference translations. There is also an added brevity penalty to avoid translations that are just really short. The equations below are for scoring texts with multiple sentences.</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>B</mi><mi>L</mi><mi>E</mi><mi>U</mi><mo>=</mo><mi>B</mi><mi>P</mi><mo>‚ãÖ</mo><mi>exp</mi><mo>‚Å°</mo><mrow><mo fence="true">(</mo><munderover><mo>‚àë</mo><mrow><mi>n</mi><mo>=</mo><mn>1</mn></mrow><mi>N</mi></munderover><msub><mi>w</mi><mi>n</mi></msub><mi>log</mi><mo>‚Å°</mo><msub><mi>p</mi><mi>n</mi></msub><mo fence="true">)</mo></mrow></mrow><annotation encoding="application/x-tex">BLEU = BP \cdot \exp \left ( \sum_{n=1}^N w_n \log{p_n} \right )</annotation></semantics></math></span><!----></div> <p>Where:</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>B</mi><mi>P</mi><mo>=</mo><mrow><mo fence="true">{</mo><mtable rowspacing="0.36em" columnalign="left left" columnspacing="1em"><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><mn>1</mn></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if¬†</mtext><mi>c</mi><mo>&gt;</mo><mi>r</mi></mrow></mstyle></mtd></mtr><mtr><mtd><mstyle scriptlevel="0" displaystyle="false"><msup><mi>e</mi><mrow><mn>1</mn><mo>‚àí</mo><mi>r</mi><mi mathvariant="normal">/</mi><mi>c</mi></mrow></msup></mstyle></mtd><mtd><mstyle scriptlevel="0" displaystyle="false"><mrow><mtext>if¬†</mtext><mi>c</mi><mo>‚â§</mo><mi>r</mi></mrow></mstyle></mtd></mtr></mtable></mrow></mrow><annotation encoding="application/x-tex">  BP = \begin{cases} 
    1 &amp; \text{if } c &gt; r \\
    e^{1-r/c} &amp; \text{if }  c \le r 
  \end{cases}</annotation></semantics></math></span><!----></div> <ul><li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><!----></span> represents the length of the candidate translation.</li> <li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi></mrow><annotation encoding="application/x-tex">r</annotation></semantics></math></span><!----></span> represents the effective corpus length, and is calculated by summing the best match lengths (best match length is just the closest reference sentence length, so if the candidate translation for a sentence has 12 words and there are references with 10 and 13 words, the best match length would be 13) for each candidate sentence in the corpus.</li></ul> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>w</mi><mi>n</mi></msub><mo>=</mo><mfrac><mn>1</mn><mi>N</mi></mfrac></mrow><annotation encoding="application/x-tex">  w_n = \frac{1}{N}</annotation></semantics></math></span><!----></div> <ul><li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><!----></span> is usually around 4</li></ul> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>p</mi><mi>n</mi></msub><mo>=</mo><mstyle scriptlevel="0" displaystyle="true"><munder><mo>‚àë</mo><mrow><mi>C</mi><mo>‚àà</mo><mo stretchy="false">{</mo><mi>C</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>s</mi><mo stretchy="false">}</mo></mrow></munder><munder><mo>‚àë</mo><mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo>‚àà</mo><mi>C</mi></mrow></munder><mfrac><mrow><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><msub><mi>t</mi><mrow><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi></mrow></msub><mo stretchy="false">(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo stretchy="false">)</mo></mrow><mrow><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">(</mo><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mo stretchy="false">)</mo></mrow></mfrac></mstyle></mrow><annotation encoding="application/x-tex">p_n = \displaystyle \sum_{C \in \{Candidates\}} \sum_{n \text{-} gram \in {C}} \frac{Count_{clip} (n \text{-} gram)}{Count (n \text{-} gram)}</annotation></semantics></math></span><!----></span></p> <ul><li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><!----></span> represents a sentence in <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>a</mi><mi>n</mi><mi>d</mi><mi>i</mi><mi>d</mi><mi>a</mi><mi>t</mi><mi>e</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">Candidates</annotation></semantics></math></span><!----></span>, which contains all the sentences in the translation. Now there is a technicality here, when I say <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><!----></span> is ‚Äúa sentence‚Äù, what I really mean is <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi></mrow><annotation encoding="application/x-tex">C</annotation></semantics></math></span><!----></span> is the translation of a sentence and could technically be multiple sentences.</li></ul> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><msub><mi>t</mi><mrow><mi>c</mi><mi>l</mi><mi>i</mi><mi>p</mi></mrow></msub><mo>=</mo><mi>min</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo separator="true">,</mo><mi>M</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>R</mi><mi>e</mi><mi>f</mi><mi mathvariant="normal">_</mi><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Count_{clip} = \min(Count, Max \_ Ref \_ Count)</annotation></semantics></math></span><!----></span></p> <ul><li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mi>a</mi><mi>x</mi><mi mathvariant="normal">_</mi><mi>R</mi><mi>e</mi><mi>f</mi><mi mathvariant="normal">_</mi><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">Max \_ Ref \_ Count</annotation></semantics></math></span><!----></span> is the maximum number of times an <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">n \text{-} gram</annotation></semantics></math></span><!----></span> is seen in a single reference translation.</li> <li><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>C</mi><mi>o</mi><mi>u</mi><mi>n</mi><mi>t</mi></mrow><annotation encoding="application/x-tex">Count</annotation></semantics></math></span><!----></span> is just the number of times an <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi></mrow><annotation encoding="application/x-tex">n \text{-} gram</annotation></semantics></math></span><!----></span> is seen in the candidate translation.</li></ul> <p>I just want to take a second to sum this all up a little more intuitively. Basically, we are going over different values of <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><!----></span> from 1 to about 4, and (in the case where we are using our simple, uniform, definition for <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>n</mi></msub></mrow><annotation encoding="application/x-tex">w_n</annotation></semantics></math></span><!----></span>) taking the average of the number of those <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">n \text{-} grams</annotation></semantics></math></span><!----></span> seen in the candidate translation and the reference translations, over the number of those <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mtext>-</mtext><mi>g</mi><mi>r</mi><mi>a</mi><mi>m</mi><mi>s</mi></mrow><annotation encoding="application/x-tex">n \text{-} grams</annotation></semantics></math></span><!----></span> in the candidate translation. Finally, we have this brevity penalty that penalizes translations that are very short. Overal this outputs a number from 0 to 1, and the higher the number is the more it is like the reference translations. This is a bit of an over simplification, but it gets the idea across.</p> <p>Now that we have some background on neural machine translation and the BLEU score, we are ready to tackle attention! We will start by understanding previous approaches to neural machine translation, and then see how the attention mechanism makes it better.</p> <h1>Previous Approach</h1> <p>The previous approach mentioned in the paper is called <a href="https://machinelearningmastery.com/encoder-decoder-recurrent-neural-network-models-neural-machine-translation/" rel="nofollow">RNN Encoder-Decoder</a>, and was proposed by <a href="https://arxiv.org/pdf/1406.1078.pdf" rel="nofollow">Cho et al.</a> and <a href="https://arxiv.org/pdf/1409.3215.pdf" rel="nofollow">Sutskever et al.</a>.</p> <p><img src="/images/transformers1/encoderDecoder.png" alt="RNN Encoder-Decoder" title="RNN Encoder-Decoder"></p> <h2>Encoder</h2> <p>Essentially, this model works by first having an encoder that reads in the input sentence <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">x</mi></mrow><annotation encoding="application/x-tex">\mathbf{x}</annotation></semantics></math></span><!----></span> and outputs a fixed-length vector <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><!----></span> called the context vector (more on this later). This can be done as follows:</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><msub><mi>h</mi><mi>t</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>x</mi><mi>t</mi></msub><mo separator="true">,</mo><msub><mi>h</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">  h_t = f(x_t, h_{t-1})</annotation></semantics></math></span><!----></div> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>c</mi><mo>=</mo><mi>q</mi><mo stretchy="false">(</mo><mo stretchy="false">{</mo><msub><mi>h</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi>h</mi><msub><mi>T</mi><mi>x</mi></msub></msub><mo stretchy="false">}</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">  c = q(\{h_1, \ldots , h_{T_x} \})</annotation></semantics></math></span><!----></div> <p>The paper describes how the previous approaches used an LSTM as <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><!----></span> and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><msub><mi>T</mi><mi>x</mi></msub></msub><mo>=</mo><mi>q</mi><mo stretchy="false">(</mo><mo stretchy="false">{</mo><msub><mi>h</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi>h</mi><msub><mi>T</mi><mi>x</mi></msub></msub><mo stretchy="false">}</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">h_{T_x} = q(\{h_1, \ldots , h_{T_x} \})</annotation></semantics></math></span><!----></span>. Hopefully, you can see how this makes sense given your background RNN understanding. It‚Äôs pretty much a many to one RNN, where we are just reading in the sentence word by word and outputting the hidden state after the final word is read in to the decoder.</p> <h2>Decoder</h2> <p>The decoder aims to predict the next word <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup></msub></mrow><annotation encoding="application/x-tex">y_{t&#x27;}</annotation></semantics></math></span><!----></span> given the context vector <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><!----></span> and the previously predicted words <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">{</mo><msub><mi>y</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi>y</mi><mrow><msup><mi>t</mi><mo mathvariant="normal" lspace="0em" rspace="0em">‚Ä≤</mo></msup><mo>‚àí</mo><mn>1</mn></mrow></msub><mo stretchy="false">}</mo></mrow><annotation encoding="application/x-tex">\{y_1, \ldots, y_{t&#x27;-1} \}</annotation></semantics></math></span><!----></span>. The decoder basically defines a probability over a translation <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">y</mi></mrow><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math></span><!----></span> that goes as follows, decomposing the joint probability into ordered conditional probabilities:</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi mathvariant="bold">y</mi><mo stretchy="false">)</mo><mo>=</mo><munderover><mo>‚àè</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>T</mi></munderover><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">‚à£</mi><mo stretchy="false">{</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo stretchy="false">}</mo><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">  p(\mathbf{y}) = \prod_{t=1}^{T} p(y_t | \{ y_1 \ldots, y_{t-1}\}, c)</annotation></semantics></math></span><!----></div> <p>RNNs allow us to model each conditional probability as:</p> <div class="math math-display"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>t</mi></msub><mi mathvariant="normal">‚à£</mi><mo stretchy="false">{</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo stretchy="false">}</mo><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">)</mo><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mrow><mi>t</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>s</mi><mi>t</mi></msub><mo separator="true">,</mo><mi>c</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">  p(y_t | \{ y_1 \ldots, y_{t-1}\}, c) = g(y_{t-1}, s_t, c)</annotation></semantics></math></span><!----></div> <p>where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><!----></span> is an RNN (GRU, LSTM, whatever) and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">s_t</annotation></semantics></math></span><!----></span> represents the hidden state. This is just a one to many RNN that takes an input context vector <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><!----></span> and outputs a translation <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="bold">y</mi></mrow><annotation encoding="application/x-tex">\mathbf{y}</annotation></semantics></math></span><!----></span>.</p> <h2>Issues</h2> <p>The biggest problem that the authors mention with this model is the fact that it uses a fixed length context vector <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><!----></span>. This forces the model to squash all the information from the input sentence, into a set length vector. The problem is this sentence can be of different lengths. The attention mechanism that is proposed in this paper frees the mode from this fixed length context vector <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><!----></span> and is shown to achieve much better performance on longer sentences.</p> <h1>New Model</h1> <p>The new mechanism that they claim learns to align and translate uses the attention mechanism and looks as follows:</p> <p><img src="/images/transformers1/attentionMechanismPersonalDiagram.png" alt="Attention Mechanism" title="Attention Mechanism"></p> <h2>Decoder</h2> <p>Instead of defining each conditional probability as in Eq. 9, we can define them as follows:</p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mi>i</mi></msub><mi mathvariant="normal">‚à£</mi><mo stretchy="false">{</mo><msub><mi>y</mi><mn>1</mn></msub><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo stretchy="false">}</mo><mo separator="true">,</mo><mi mathvariant="bold">x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>g</mi><mo stretchy="false">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>s</mi><mi>i</mi></msub><mo separator="true">,</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(y_i | \{ y_1 \ldots, y_{i-1}\}, \mathbf{x}) = g(y_{i-1}, s_i, c_i)</annotation></semantics></math></span><!----></span></p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>g</mi></mrow><annotation encoding="application/x-tex">g</annotation></semantics></math></span><!----></span> is just some RNN model again. To output a word <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">y_i</annotation></semantics></math></span><!----></span>, you just need to sample from the conditional distribution.</p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub><mo>=</mo><mi>f</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>y</mi><mrow><mi>i</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>c</mi><mi>i</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">s_i = f(s_{i-1}, y_{i-1}, c_i)</annotation></semantics></math></span><!----></span></p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">s_i</annotation></semantics></math></span><!----></span> is just the hidden state at time <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>, and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi></mrow><annotation encoding="application/x-tex">f</annotation></semantics></math></span><!----></span> is another part of whatever RNN architecture is being used.</p> <p><em>Note how the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><!----></span> term (context vector) now has a subscript!</em> This is because the context vector <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">c_i</annotation></semantics></math></span><!----></span> term now depends on a sequence of annotations <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi>h</mi><msub><mi>T</mi><mi>x</mi></msub></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(h_1, \ldots, h_{T_x})</annotation></semantics></math></span><!----></span> which the encoder creates. <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">c_i</annotation></semantics></math></span><!----></span> is a weighted sum of these annotations.</p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub><mo>=</mo><msubsup><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>x</mi></msub></msubsup><msub><mi>Œ±</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j</annotation></semantics></math></span><!----></span></p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><mi>exp</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="false">)</mo></mrow><mrow><msubsup><mo>‚àë</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>T</mi><mi>x</mi></msub></msubsup><mi>exp</mi><mo>‚Å°</mo><mo stretchy="false">(</mo><msub><mi>e</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="false">)</mo></mrow></mfrac><mo>=</mo><mi>œÉ</mi><mo stretchy="false">(</mo><msub><mi>e</mi><mi>i</mi></msub><msub><mo stretchy="false">)</mo><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">\alpha_{ij} = \frac{\exp(e_{ij})}{\sum_{k=1}^{T_x} \exp(e_{ik})} = \sigma(e_{i})_j</annotation></semantics></math></span><!----></span></p> <p>Where <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÉ</mi></mrow><annotation encoding="application/x-tex">\sigma</annotation></semantics></math></span><!----></span> is just the softmax function, and:</p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mi>a</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>i</mi><mo>‚àí</mo><mn>1</mn></mrow></msub><mo separator="true">,</mo><msub><mi>h</mi><mi>j</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">e_{ij} = a(s_{i-1}, h_j)</annotation></semantics></math></span><!----></span></p> <p><span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><!----></span> is an alignment model which scores how well inputs around position j in the candidate sentence and the output translation at i match. Here <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><!----></span> is just a feedforward neural network that is trained jointly with everything else.</p> <p>While this may all look scary at first, all that‚Äôs really going on is that we are using <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">c_i</annotation></semantics></math></span><!----></span> for the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>th output word instead of <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><!----></span>. And we are calculating <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>c</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">c_i</annotation></semantics></math></span><!----></span> as a weighted sum of these annotations <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><!----></span> that the encoder creates. <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">h_j</annotation></semantics></math></span><!----></span> contains information about the whole input sentence with a focus on the parts surrounding the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><!----></span>th word.</p> <p><strong>Pay attention to the attention mechanism here üòâ!!</strong> These weights <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{ij}</annotation></semantics></math></span><!----></span> tell the model what parts of the sentence to pay attention to when predicting the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>th word. The alignment model figures out what parts of the sentence to pay attention to based on the previous hidden state <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mrow><mi>i</mi><mo>‚àí</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">s_{i-1}</annotation></semantics></math></span><!----></span>.</p> <p>The cool thing is this is kind of similar to what we people do as well. If I had to translate a short sentence from English to German (I don‚Äôt know French but I do know German), I could just read the entire sentence (encoder), store it in my brain (context vector <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><!----></span>), and then say it in German (decoder). But if I had to translate a longer sentence, I would first try to translate the beginning of the English sentence, and pay attention to the words over there. I wouldn‚Äôt care about the words at the end of the sentence to translate the ones at the beginning.</p> <h2>Encoder</h2> <p>The encoder is responsible for creating these annotations <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><msub><mi>h</mi><mn>1</mn></msub><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><msub><mi>h</mi><msub><mi>T</mi><mi>x</mi></msub></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(h_1, \ldots, h_{T_x})</annotation></semantics></math></span><!----></span> and does so using a <a href="https://ieeexplore.ieee.org/document/650093" rel="nofollow">bidirectional RNN</a> (BiRNN). The reason a regular RNN is not used is because a regular RNN only has information about the previous words, which wouldn‚Äôt allow annotations to have context that comes later in the sentence that could be important to the meaning of the current part of the sentence.</p> <p>The BiRNN consists of a forward and backward RNNs that read the sentences in regular and reverse order respectively and calculate the hidden states <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mover accent="true"><msub><mi>h</mi><mn>1</mn></msub><mo stretchy="true">‚Üí</mo></mover><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><mover accent="true"><msub><mi>h</mi><msub><mi>T</mi><mi>x</mi></msub></msub><mo stretchy="true">‚Üí</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\overrightarrow{h_1}, \ldots, \overrightarrow{h_{T_x}})</annotation></semantics></math></span><!----></span> and <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mover accent="true"><msub><mi>h</mi><mn>1</mn></msub><mo stretchy="true">‚Üê</mo></mover><mo separator="true">,</mo><mo>‚Ä¶</mo><mo separator="true">,</mo><mover accent="true"><msub><mi>h</mi><msub><mi>T</mi><mi>x</mi></msub></msub><mo stretchy="true">‚Üê</mo></mover><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\overleftarrow{h_1}, \ldots, \overleftarrow{h_{T_x}})</annotation></semantics></math></span><!----></span> respectively. The arrows are just used to represent the forward and backward RNN hidden states. These are concatenated such that the hidden state <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">h_j</annotation></semantics></math></span><!----></span> corresponding to word <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>x</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">x_j</annotation></semantics></math></span><!----></span> is <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo fence="true">[</mo><mover accent="true"><msubsup><mi>h</mi><mi>j</mi><mi>T</mi></msubsup><mo stretchy="true">‚Üí</mo></mover><mo separator="true">;</mo><mover accent="true"><msubsup><mi>h</mi><mi>j</mi><mi>T</mi></msubsup><mo stretchy="true">‚Üê</mo></mover><mo fence="true">]</mo></mrow><annotation encoding="application/x-tex">\left [ \overrightarrow{h_j^T} ; \overleftarrow{h_j^T} \right ]</annotation></semantics></math></span><!----></span>. This way <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>h</mi><mi>j</mi></msub></mrow><annotation encoding="application/x-tex">h_j</annotation></semantics></math></span><!----></span> contains context from before and after in the sentence, mainly focused near the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><!----></span>th word because of RNNs tendency to represent recent inputs better.</p> <h1>Performance</h1> <p>The authors of the paper trained 4 different models: RNNencdec30, RNNencdec50, RNNsearch30, and RNNsearch50, where RNNencdec is the previous model and RNNsearch is the new attention based model, and the number at the end represents the longest sentence size seen in the training data. The graph below shows the BLEU scores for these different models on different test sentence lengths. Please note how the BLEU score is greater than 1 here, it was just multiplied by 100.</p> <p><img src="/images/transformers1/attentionMechanismPerformance.png" alt="Attention Mechanism Performance" title="Attention Mechanism Performance"></p> <p>The impressive thing here is that the performance for the attention based model has better performance across all sentence lengths. It is intuitively to be expected that it performs better for longer sentences given that the model avoids the fixed-length context vector <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>c</mi></mrow><annotation encoding="application/x-tex">c</annotation></semantics></math></span><!----></span>, but the fact that it performs better across the board is really impressive.</p> <p>Going back to our previous example from the start of the post we can see how RNNencdec-50 fails at longer sentences:</p> <p><strong>English Input Sentence:</strong></p> <blockquote><p>An admitting privilege is the right of a doctor to admit a patient to a hospital or a medical centre to carry out a diagnosis or a procedure, based on his status as a health care worker at a hospital.</p></blockquote> <p><strong>RNNencdec-50:</strong></p> <blockquote><p>Un privilege d‚Äôadmission est le droit d‚Äôun medecin de reconnaitre un patient a l‚Äôhopital ou un centre medical d‚Äôun diagnostic ou de prendre un diagnostic en
fonction de son etat de sante.</p></blockquote> <p><strong>RNNencdec-50 Translated Back Using Google Translate:</strong></p> <blockquote><p>An admitting privilege is the right of a physician to recognize a patient at hospital or medical center for a diagnosis or to take a diagnosis by depending on his state of health.</p></blockquote> <p>This translation is clearly not right at the end. But the RNNsearch-50 translation on the other hand‚Ä¶</p> <p><strong>RNNsearch-50:</strong></p> <blockquote><p>Un privilege d‚Äôadmission est le droit d‚Äôun medecin d‚Äôadmettre un patient a un hopital ou un centre medical pour effectuer un diagnostic ou une procedure, selon son statut de travailleur des soins de sante a l‚Äôhopital.</p></blockquote> <p><strong>RNNsearch-50 Translated Back Using Google Translate:</strong></p> <blockquote><p>An admitting privilege is the right of a physician to admit a patient to an hospital or medical center to perform a diagnosis or procedure, depending on his status as a health care worker at the hospital.</p></blockquote> <p>Impressive huh?</p> <p>The paper also shows these plots showing the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">\alpha_{ij}</annotation></semantics></math></span><!----></span> of the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi></mrow><annotation encoding="application/x-tex">j</annotation></semantics></math></span><!----></span>th source word for the <span class="math math-inline"><!----><span class="katex"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>i</mi></mrow><annotation encoding="application/x-tex">i</annotation></semantics></math></span><!----></span>th target word. You can see how the model learned to pay attention to things that are intuitive for people as well. ‚ÄúDestruction‚Äù and ‚ÄúLa destruction‚Äù go together but ‚ÄúSyria‚Äù for example isn‚Äôt important for that translation.</p> <p><img src="/images/transformers1/attentionMechanismAlignments.png" alt="Attention Mechanism Alignments" title="Attention Mechanism Alignments"></p> <h1>Conclusion</h1> <p>Attention is really cool and has revolutionalized much of AI. This post has reviewed the paper that first introduced attention, and introduced what is today known as additive attention. The clever way the alignment model is tied into the RNNs and to avoid the problems with fixed length context vectors and improve performance for longer input sentences is really impressive. Also the way attention is at least somewhat more biologically plausible than creating fixed length context vectors is exciting. The next post is a look at the technical details of Transformers.</p> <h1>References:</h1> <ul><li><a href="https://arxiv.org/pdf/1409.0473.pdf" rel="nofollow">Neural Machine Translation by Jointly Learning to Align and Translate</a></li> <li><a href="https://aclanthology.org/P02-1040.pdf" rel="nofollow">BLEU Score</a></li> <li><a href="https://github.com/bentrevett/pytorch-seq2seq/blob/master/3%20-%20Neural%20Machine%20Translation%20by%20Jointly%20Learning%20to%20Align%20and%20Translate.ipynb" rel="nofollow">Someone Else‚Äôs Implementation</a></li> <li><a href="https://distill.pub/2016/augmented-rnns/#attentional-interfaces" rel="nofollow">Slightly less mathmatical but prettier explanation</a></li></ul><!----><!----></div></div><!----><!--]--> <!--[!--><!--]--><!--]-->
			
			<script>
				{
					__sveltekit_94jwj8 = {
						base: new URL("..", location).pathname.slice(0, -1)
					};

					const element = document.currentScript.parentElement;

					Promise.all([
						import("../_app/immutable/entry/start.DI11qAQK.js"),
						import("../_app/immutable/entry/app.D0zaimvy.js")
					]).then(([kit, app]) => {
						kit.start(app, element, {
							node_ids: [0, 6],
							data: [null,null],
							form: null,
							error: null
						});
					});
				}
			</script>
		</div>
	</body>
</html>
