import{t as e,a}from"../chunks/DKJMLC42.js";import"../chunks/BntcpoEZ.js";import{n as t}from"../chunks/BuAUvOro.js";var r=e('<p>One way I think about deep learning, inspired by discussions with <a href="https://dkarkada.xyz/" rel="nofollow">Dhruva</a>, <a href="https://github.com/JoeyTurn/" rel="nofollow">Joey</a>, and <a href="https://james-simon.github.io/" rel="nofollow">Jamie</a>, is that it’s just a combination of hyper-parameter selection, a model, an optimizer, and data. That’s roughly how this page is organized. Links will be added as I read them and create ntoes</p> <h1>Core</h1> <h2>Hyper-parameter selection</h2> <p>This section is about scaling, initialization, and hyper-parameter selection.</p> <ul><li>LeCun init</li> <li>Hyper param transfer <ul><li>muP read TP4</li></ul></li> <li>Scaling laws</li></ul> <h2>Models</h2> <p>This section is based on the pareto frontier of models of deep learning from <a href="https://james-simon.github.io" rel="nofollow">Jamie’s</a> thesis. I am just going down the frontier from least realistic to most realistic. <img src="/images/digitalGarden/paretoFrontier.png" alt="Pareto frontier" title="Pareto Frontier"></p> <ul><li>Linear regression</li> <li>Kernel regression</li> <li>NNGP / NTK <ul><li><a href="/digitalGarden/NNGP">Deep Neural Networks as Gaussian Processes notes</a></li> <li>Jacot’s paper</li> <li>The lazy (NTK) and rich (µP) regimes: A gentle tutorial</li> <li>On Lazy Training in Differentiable Programming</li></ul></li> <li>Linear networks <ul><li>The Implicit Bias of Gradient Descent on Separable Data</li> <li>Towards Resolving the Implicit Bias of Gradient Descent for Matrix Factorization: Greedy Low-Rank Learning</li> <li>Saddle-to-Saddle Dynamics in Deep Linear Networks: Small Initialization Training, Symmetry, and Sparsity</li> <li>Neural networks and principal component analysis: Learning from examples without local minima.</li></ul></li> <li>RFMs <ul><li>Average gradient outer product as a mechanism for deep neural collapse</li></ul></li> <li>Mean field / muP <ul><li>Look at mei montanari theo + his lecture notes</li> <li>6 lectures on linearized networks</li> <li>Maybe watch talks</li> <li>TP4</li></ul></li> <li>MLPs <ul><li>Scaling MLPs: A Tale of Inductive Bias</li> <li>On the non-universality of deep learning: quantifying the cost of symmetry</li> <li>SGD learning on neural networks: leap complexity and saddle-to-saddle dynamics</li> <li>Feature emergence via margin maximization: case studies in algebraic tasks</li> <li>Memorization capacity</li> <li>SCALING LAWS FOR ASSOCIATIVE MEMORIES</li> <li>Learning Associative Memories with Gradient Descent</li> <li>Find stuff about storage capacity in models</li></ul></li> <li>Transformers <ul><li>Transformers Learn Shortcuts to Automata</li> <li>Find stuff about storage capacity in models</li> <li>Clayton representational work</li> <li>Will Merrill smth</li></ul></li> <li>SSM <ul><li>Expressivity limitations</li> <li>Modifying by idk smth</li> <li>Figure of variants of SSMs</li> <li>Comp eff on GPU (Damek)</li></ul></li></ul> <h2>Optimization</h2> <ul><li>Understanding all the optimizers <ul><li>SGD <ul><li>Hidden Progress in Deep Learning: SGD Learns Parities Near the Computational Limit</li></ul></li> <li>RMS Prop</li> <li>Momentum</li> <li>Adam + W</li> <li>Muon</li></ul></li> <li>EoS <ul><li>Self-Stabilization: The Implicit Bias of Gradient Descent at the Edge of Stability</li> <li>Central flows</li></ul></li> <li>Loss spikes <ul><li>Small-scale proxies for large-scale Transformer training instabilities</li></ul></li> <li>Other tricks <ul><li>ADDING GRADIENT NOISE IMPROVES LEARNING FOR VERY DEEP NETWORKS</li></ul></li></ul> <h2>Data</h2> <ul><li>Pre-training data distribution</li> <li>Post-training data distribution</li></ul> <h1>Other</h1> <h2>Hardware aware</h2> <ul><li>Albert Gu Flash attention</li> <li>Horace He</li> <li>Dion: Distributed Orthonormalized Updates</li> <li>The Polar Express: Optimal Matrix Sign Methods and Their Application to the Muon Algorithm</li></ul> <h2>Distillation</h2> <ul><li>Idk</li></ul> <h2>Post-training</h2> <h2>Mechanistic interpretability</h2> <h2>What does it mean to understand? What are we looking for from a theory of deep learning? What could be a unified theory of deep learning?</h2> <p>Eliminating hyper parameters, very good theory empirics match to show we fully understood everything</p> <h1>References</h1> <ul><li><a href="https://analyticinterp.github.io/" rel="nofollow">https://analyticinterp.github.io/</a></li> <li><img src="/images/digitalGarden/theGraph.png" alt="The Graph" title="The Graph"></li> <li><a href="https://surbhi18.github.io/FoMML/calendar/" rel="nofollow">https://surbhi18.github.io/FoMML/calendar/</a></li> <li><a href="https://blakebordelon.github.io/summary.html" rel="nofollow">https://blakebordelon.github.io/summary.html</a></li></ul>',1);function h(i){var l=r();t(44),a(i,l)}export{h as component};
